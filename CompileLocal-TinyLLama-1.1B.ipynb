{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "342cf85b-2dda-4132-9d6e-4c3be8a918c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: optimum[neuronx] in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (1.20.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from optimum[neuronx]) (15.0.1)\n",
      "Requirement already satisfied: sympy in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from optimum[neuronx]) (1.12)\n",
      "Requirement already satisfied: transformers<4.42.0,>=4.26.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum[neuronx]) (4.40.0)\n",
      "Requirement already satisfied: torch>=1.11 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from optimum[neuronx]) (2.1.2)\n",
      "Requirement already satisfied: packaging in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from optimum[neuronx]) (21.3)\n",
      "Requirement already satisfied: numpy in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from optimum[neuronx]) (1.25.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from optimum[neuronx]) (0.23.2)\n",
      "Requirement already satisfied: datasets in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from optimum[neuronx]) (2.19.2)\n",
      "Requirement already satisfied: optimum-neuron>=0.0.20 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.0.23)\n",
      "Requirement already satisfied: filelock in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neuronx]) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neuronx]) (2024.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neuronx]) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neuronx]) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neuronx]) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neuronx]) (4.12.0)\n",
      "Collecting transformers<4.42.0,>=4.26.0 (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum[neuronx])\n",
      "  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: accelerate==0.29.2 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from optimum-neuron>=0.0.20->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.29.2)\n",
      "Requirement already satisfied: protobuf<4 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from optimum-neuron>=0.0.20->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (3.19.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from transformers<4.42.0,>=4.26.0->transformers[sentencepiece]<4.42.0,>=4.26.0->optimum[neuronx]) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from transformers<4.42.0,>=4.26.0->transformers[sentencepiece]<4.42.0,>=4.26.0->optimum[neuronx]) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from transformers<4.42.0,>=4.26.0->transformers[sentencepiece]<4.42.0,>=4.26.0->optimum[neuronx]) (0.4.3)\n",
      "Requirement already satisfied: psutil in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from accelerate==0.29.2->optimum-neuron>=0.0.20->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (5.9.8)\n",
      "Requirement already satisfied: wheel in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.43.0)\n",
      "Requirement already satisfied: neuronx-cc==2.13.66.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (2.13.66.0+6dfecc895)\n",
      "Requirement already satisfied: torch-neuronx==2.1.2.2.1.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (2.1.2.2.1.0)\n",
      "Requirement already satisfied: transformers-neuronx==0.10.0.21 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.10.0.21)\n",
      "Requirement already satisfied: torchvision==0.16.* in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.16.2)\n",
      "Requirement already satisfied: neuronx-distributed==0.7.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.7.0)\n",
      "Requirement already satisfied: networkx in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch>=1.11->optimum[neuronx]) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch>=1.11->optimum[neuronx]) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch>=1.11->optimum[neuronx]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch>=1.11->optimum[neuronx]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch>=1.11->optimum[neuronx]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch>=1.11->optimum[neuronx]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch>=1.11->optimum[neuronx]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch>=1.11->optimum[neuronx]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch>=1.11->optimum[neuronx]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch>=1.11->optimum[neuronx]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch>=1.11->optimum[neuronx]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch>=1.11->optimum[neuronx]) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch>=1.11->optimum[neuronx]) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch>=1.11->optimum[neuronx]) (2.1.0)\n",
      "Requirement already satisfied: scipy<=1.11.2 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from neuronx-cc==2.13.66.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (1.11.2)\n",
      "Requirement already satisfied: python-daemon>=2.2.4 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from neuronx-cc==2.13.66.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (3.0.1)\n",
      "Requirement already satisfied: requests-unixsocket>=0.1.5 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from neuronx-cc==2.13.66.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.3.0)\n",
      "Requirement already satisfied: islpy<=2023.1,>2021.1 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from neuronx-cc==2.13.66.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (2023.1)\n",
      "Requirement already satisfied: pgzip>=0.3.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from neuronx-cc==2.13.66.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.3.5)\n",
      "Requirement already satisfied: ec2-metadata<=2.10.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from neuronx-cc==2.13.66.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (2.10.0)\n",
      "Requirement already satisfied: torch-xla in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from neuronx-distributed==0.7.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (2.1.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum[neuronx]) (12.5.40)\n",
      "Requirement already satisfied: libneuronxla<3.0,>2.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch-neuronx==2.1.2.2.1.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (2.0.965)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torchvision==0.16.*->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (10.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch-xla->neuronx-distributed==0.7.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (2.1.0)\n",
      "Requirement already satisfied: cloud-tpu-client>=0.10.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from torch-xla->neuronx-distributed==0.7.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.10)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from packaging->optimum[neuronx]) (3.1.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum[neuronx]) (0.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from coloredlogs->optimum[neuronx]) (10.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from datasets->optimum[neuronx]) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from datasets->optimum[neuronx]) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from datasets->optimum[neuronx]) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from datasets->optimum[neuronx]) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from datasets->optimum[neuronx]) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from datasets->optimum[neuronx]) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from datasets->optimum[neuronx]) (3.9.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from sympy->optimum[neuronx]) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neuronx]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neuronx]) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neuronx]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neuronx]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neuronx]) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neuronx]) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum[neuronx]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum[neuronx]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum[neuronx]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum[neuronx]) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum[neuronx]) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from pandas->datasets->optimum[neuronx]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from pandas->datasets->optimum[neuronx]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from pandas->datasets->optimum[neuronx]) (2024.1)\n",
      "Requirement already satisfied: aws-neuronx-runtime-discovery~=2.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from libneuronxla<3.0,>2.0->torch-neuronx==2.1.2.2.1.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (2.9)\n",
      "Requirement already satisfied: boto3 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from libneuronxla<3.0,>2.0->torch-neuronx==2.1.2.2.1.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (1.34.113)\n",
      "Requirement already satisfied: botocore in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from libneuronxla<3.0,>2.0->torch-neuronx==2.1.2.2.1.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (1.34.113)\n",
      "Requirement already satisfied: docutils in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from python-daemon>=2.2.4->neuronx-cc==2.13.66.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.16)\n",
      "Requirement already satisfied: lockfile>=0.10 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from python-daemon>=2.2.4->neuronx-cc==2.13.66.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.12.2)\n",
      "Requirement already satisfied: setuptools>=62.4.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from python-daemon>=2.2.4->neuronx-cc==2.13.66.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (70.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum[neuronx]) (1.16.0)\n",
      "Requirement already satisfied: google-api-python-client==1.8.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.7.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (1.8.0)\n",
      "Requirement already satisfied: oauth2client in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.7.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (4.1.3)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.7.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.22.0)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.7.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (2.29.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.7.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.2.0)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.7.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (1.34.1)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.7.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (3.0.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from boto3->libneuronxla<3.0,>2.0->torch-neuronx==2.1.2.2.1.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from boto3->libneuronxla<3.0,>2.0->torch-neuronx==2.1.2.2.1.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.10.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.7.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.6.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.7.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.4.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.7.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (4.7.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.7.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (1.63.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.7.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (5.3.3)\n",
      "Downloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.40.0\n",
      "    Uninstalling transformers-4.40.0:\n",
      "      Successfully uninstalled transformers-4.40.0\n",
      "Successfully installed transformers-4.41.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip show transformers tokenizers sentencepiece torch-neuronx neuronx-cc transformers-neuronx\n",
    "!pip install --upgrade-strategy eager optimum[neuronx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cee89c0-0748-4521-aebf-73dc9b6a008e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d4a13fe56e452283c6bbf276bbbfc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664dbf05c5bd483d88fd9e478d4a1bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_transformers_neuronx/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0413f9e6e9b4960b8a6179cb823fa20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 04:30:55.000073:  3712  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-06-18 04:31:06.000833:  4220  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-06-18 04:31:06.000871:  4221  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-06-18 04:31:06.000951:  4223  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-06-18 04:31:06.000911:  4222  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-06-18 04:31:06.000988:  4224  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-06-18 04:31:07.000022:  4225  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_2b40013e166e9d3f79f4+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_ac5dbdb82ea09dcb1c02+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_c4996acaf5e4f18bbb64+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_4565a123a64f7cf181bf+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_82c62c710aede9f5b02a+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_c4996acaf5e4f18bbb64+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_ac5dbdb82ea09dcb1c02+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_2b40013e166e9d3f79f4+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_4565a123a64f7cf181bf+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_82c62c710aede9f5b02a+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_ac5dbdb82ea09dcb1c02+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 04:31:08.000139:  4224  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --target=trn1 --framework=XLA /tmp/ubuntu/neuroncc_compile_workdir/7264ffa6-7038-4616-b350-45a7af8613b7/model.MODULE_ac5dbdb82ea09dcb1c02+2c2d707e.hlo_module.pb --output /tmp/ubuntu/neuroncc_compile_workdir/7264ffa6-7038-4616-b350-45a7af8613b7/model.MODULE_ac5dbdb82ea09dcb1c02+2c2d707e.neff --model-type=transformer --auto-cast=none --verbose=35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_2b40013e166e9d3f79f4+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 04:31:08.000145:  4223  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --target=trn1 --framework=XLA /tmp/ubuntu/neuroncc_compile_workdir/b7bcfa75-8cfe-4417-b647-c8b477da59d9/model.MODULE_2b40013e166e9d3f79f4+2c2d707e.hlo_module.pb --output /tmp/ubuntu/neuroncc_compile_workdir/b7bcfa75-8cfe-4417-b647-c8b477da59d9/model.MODULE_2b40013e166e9d3f79f4+2c2d707e.neff --model-type=transformer --auto-cast=none --verbose=35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_c4996acaf5e4f18bbb64+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 04:31:08.000148:  4220  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --target=trn1 --framework=XLA /tmp/ubuntu/neuroncc_compile_workdir/5e63bf41-20fe-4ed8-b233-cc21a6f37508/model.MODULE_c4996acaf5e4f18bbb64+2c2d707e.hlo_module.pb --output /tmp/ubuntu/neuroncc_compile_workdir/5e63bf41-20fe-4ed8-b233-cc21a6f37508/model.MODULE_c4996acaf5e4f18bbb64+2c2d707e.neff --model-type=transformer --auto-cast=none --verbose=35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_4565a123a64f7cf181bf+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_82c62c710aede9f5b02a+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 04:31:08.000164:  4222  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --target=trn1 --framework=XLA /tmp/ubuntu/neuroncc_compile_workdir/2fa0af4b-ecb6-4163-9cf2-1d0751caf890/model.MODULE_4565a123a64f7cf181bf+2c2d707e.hlo_module.pb --output /tmp/ubuntu/neuroncc_compile_workdir/2fa0af4b-ecb6-4163-9cf2-1d0751caf890/model.MODULE_4565a123a64f7cf181bf+2c2d707e.neff --model-type=transformer --auto-cast=none --verbose=35\n",
      "2024-06-18 04:31:08.000165:  4221  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --target=trn1 --framework=XLA /tmp/ubuntu/neuroncc_compile_workdir/23a94bc0-620a-4092-b943-ab59373387f6/model.MODULE_82c62c710aede9f5b02a+2c2d707e.hlo_module.pb --output /tmp/ubuntu/neuroncc_compile_workdir/23a94bc0-620a-4092-b943-ab59373387f6/model.MODULE_82c62c710aede9f5b02a+2c2d707e.neff --model-type=transformer --auto-cast=none --verbose=35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_87075a6ccebd8bc1b197+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_87075a6ccebd8bc1b197+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neuronxcc-2.13.66.0+6dfecc895/MODULE_87075a6ccebd8bc1b197+2c2d707e/model.neff not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 04:31:09.000781:  4225  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --target=trn1 --framework=XLA /tmp/ubuntu/neuroncc_compile_workdir/3c58b4e7-4e0f-40ed-983d-193834f7b06c/model.MODULE_87075a6ccebd8bc1b197+2c2d707e.hlo_module.pb --output /tmp/ubuntu/neuroncc_compile_workdir/3c58b4e7-4e0f-40ed-983d-193834f7b06c/model.MODULE_87075a6ccebd8bc1b197+2c2d707e.neff --model-type=transformer --auto-cast=none --verbose=35\n",
      ".............performing partition vectorization on AG_2[[0, 3, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_3_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 6, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_8_TC_SRC, dag_85_TRANSPOSE_SRC, dag_62, dag_6, dag_61}\n",
      "performing partition vectorization on AG_2[[0, 58, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_58_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 95, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_95_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 98, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_98, dag_104, dag_99_TC_SRC, dag_105, dag_127_TRANSPOSE_SRC}\n",
      "performing partition vectorization on AG_2[[0, 101, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_101_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 138, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_138_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 141, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_148, dag_142_TC_SRC, dag_170_TRANSPOSE_SRC, dag_141, dag_147}\n",
      "performing partition vectorization on AG_2[[0, 144, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_144_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 181, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_181_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 184, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_184, dag_190, dag_213_TRANSPOSE_SRC, dag_191, dag_185_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 187, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_187_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 224, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_224_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 227, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_227, dag_233, dag_256_TRANSPOSE_SRC, dag_228_TC_SRC, dag_234}\n",
      "performing partition vectorization on AG_2[[0, 230, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_230_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 267, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_267_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 270, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_299_TRANSPOSE_SRC, dag_277, dag_271_TC_SRC, dag_270, dag_276}\n",
      "performing partition vectorization on AG_2[[0, 273, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_273_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 310, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_310_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 313, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_313, dag_319, dag_320, dag_314_TC_SRC, dag_342_TRANSPOSE_SRC}\n",
      "performing partition vectorization on AG_2[[0, 316, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_316_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 353, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_353_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 356, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_363, dag_357_TC_SRC, dag_385_TRANSPOSE_SRC, dag_356, dag_362}\n",
      "performing partition vectorization on AG_2[[0, 359, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_359_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 396, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_396_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 399, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_400_TC_SRC, dag_399, dag_405, dag_428_TRANSPOSE_SRC, dag_406}\n",
      "performing partition vectorization on AG_2[[0, 402, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_402_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 439, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_439_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 442, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_471_TRANSPOSE_SRC, dag_449, dag_443_TC_SRC, dag_442, dag_448}\n",
      "performing partition vectorization on AG_2[[0, 445, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_445_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 482, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_482_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 485, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_514_TRANSPOSE_SRC, dag_485, dag_486_TC_SRC, dag_491, dag_492}\n",
      "performing partition vectorization on AG_2[[0, 488, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_488_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 525, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_525_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 528, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_529_TC_SRC, dag_535, dag_557_TRANSPOSE_SRC, dag_528, dag_534}\n",
      "performing partition vectorization on AG_2[[0, 531, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_531_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 568, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_568_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 571, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_571, dag_577, dag_600_TRANSPOSE_SRC, dag_572_TC_SRC, dag_578}\n",
      "performing partition vectorization on AG_2[[0, 574, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_574_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 611, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_611_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 614, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_620, dag_615_TC_SRC, dag_621, dag_643_TRANSPOSE_SRC, dag_614}\n",
      "performing partition vectorization on AG_2[[0, 617, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_617_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 654, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_654_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 657, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_686_TRANSPOSE_SRC, dag_657, dag_663, dag_664, dag_658_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 660, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_660_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 697, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_697_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 700, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_706, dag_707, dag_701_TC_SRC, dag_729_TRANSPOSE_SRC, dag_700}\n",
      "performing partition vectorization on AG_2[[0, 703, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_703_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 740, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_740_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 743, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_744_TC_SRC, dag_750, dag_743, dag_772_TRANSPOSE_SRC, dag_749}\n",
      "performing partition vectorization on AG_2[[0, 746, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_746_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 783, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_783_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 786, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_786, dag_792, dag_815_TRANSPOSE_SRC, dag_793, dag_787_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 789, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_789_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 826, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_826_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 829, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_836, dag_858_TRANSPOSE_SRC, dag_830_TC_SRC, dag_829, dag_835}\n",
      "performing partition vectorization on AG_2[[0, 832, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_832_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 869, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_869_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 872, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_873_TC_SRC, dag_901_TRANSPOSE_SRC, dag_872, dag_878, dag_879}\n",
      "performing partition vectorization on AG_2[[0, 875, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_875_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 3, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_3_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 6, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_8_TC_SRC, dag_85_TRANSPOSE_SRC, dag_62, dag_6, dag_61}\n",
      "performing partition vectorization on AG_2[[0, 58, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_58_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 95, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_95_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 98, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_98, dag_104, dag_99_TC_SRC, dag_105, dag_127_TRANSPOSE_SRC}\n",
      "performing partition vectorization on AG_2[[0, 101, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_101_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 138, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_138_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 141, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_148, dag_142_TC_SRC, dag_170_TRANSPOSE_SRC, dag_141, dag_147}\n",
      "performing partition vectorization on AG_2[[0, 144, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_144_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 181, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_181_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 184, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_184, dag_190, dag_213_TRANSPOSE_SRC, dag_191, dag_185_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 187, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_187_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 224, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_224_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 227, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_227, dag_256_TRANSPOSE_SRC, dag_233, dag_228_TC_SRC, dag_234}\n",
      "performing partition vectorization on AG_2[[0, 230, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_230_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 267, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_267_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 270, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_299_TRANSPOSE_SRC, dag_277, dag_271_TC_SRC, dag_270, dag_276}\n",
      "performing partition vectorization on AG_2[[0, 273, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_273_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 310, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_310_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 313, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_313, dag_319, dag_320, dag_314_TC_SRC, dag_342_TRANSPOSE_SRC}\n",
      "performing partition vectorization on AG_2[[0, 316, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_316_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 353, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_353_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 356, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_363, dag_357_TC_SRC, dag_385_TRANSPOSE_SRC, dag_356, dag_362}\n",
      "performing partition vectorization on AG_2[[0, 359, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_359_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 396, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_396_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 399, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_400_TC_SRC, dag_399, dag_428_TRANSPOSE_SRC, dag_405, dag_406}\n",
      "performing partition vectorization on AG_2[[0, 402, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_402_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 439, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_439_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 442, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_471_TRANSPOSE_SRC, dag_449, dag_443_TC_SRC, dag_442, dag_448}\n",
      "performing partition vectorization on AG_2[[0, 445, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_445_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 482, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_482_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 485, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_514_TRANSPOSE_SRC, dag_485, dag_486_TC_SRC, dag_491, dag_492}\n",
      "performing partition vectorization on AG_2[[0, 488, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_488_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 525, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_525_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 528, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_529_TC_SRC, dag_535, dag_557_TRANSPOSE_SRC, dag_528, dag_534}\n",
      "performing partition vectorization on AG_2[[0, 531, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_531_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 568, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_568_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 571, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_571, dag_600_TRANSPOSE_SRC, dag_577, dag_572_TC_SRC, dag_578}\n",
      "performing partition vectorization on AG_2[[0, 574, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_574_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 611, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_611_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 614, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_620, dag_615_TC_SRC, dag_621, dag_643_TRANSPOSE_SRC, dag_614}\n",
      "performing partition vectorization on AG_2[[0, 617, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_617_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 654, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_654_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 657, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_686_TRANSPOSE_SRC, dag_657, dag_663, dag_664, dag_658_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 660, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_660_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 697, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_697_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 700, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_706, dag_707, dag_701_TC_SRC, dag_729_TRANSPOSE_SRC, dag_700}\n",
      "performing partition vectorization on AG_2[[0, 703, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_703_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 740, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_740_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 743, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_744_TC_SRC, dag_750, dag_743, dag_772_TRANSPOSE_SRC, dag_749}\n",
      "performing partition vectorization on AG_2[[0, 746, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_746_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 783, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_783_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 786, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_786, dag_792, dag_815_TRANSPOSE_SRC, dag_793, dag_787_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 789, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_789_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 826, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_826_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 829, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_836, dag_858_TRANSPOSE_SRC, dag_830_TC_SRC, dag_829, dag_835}\n",
      "performing partition vectorization on AG_2[[0, 832, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_832_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 869, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_869_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 872, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_873_TC_SRC, dag_901_TRANSPOSE_SRC, dag_872, dag_878, dag_879}\n",
      "performing partition vectorization on AG_2[[0, 875, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_875_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 3, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_3_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 6, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_8_TC_SRC, dag_85_TRANSPOSE_SRC, dag_62, dag_6, dag_61}\n",
      "performing partition vectorization on AG_2[[0, 58, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_58_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 95, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_95_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 98, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_98, dag_104, dag_99_TC_SRC, dag_105, dag_127_TRANSPOSE_SRC}\n",
      "performing partition vectorization on AG_2[[0, 101, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_101_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 138, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_138_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 141, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_142_TC_SRC, dag_148, dag_170_TRANSPOSE_SRC, dag_141, dag_147}\n",
      "performing partition vectorization on AG_2[[0, 144, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_144_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 181, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_181_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 184, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_184, dag_213_TRANSPOSE_SRC, dag_190, dag_191, dag_185_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 187, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_187_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 224, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_224_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 227, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_227, dag_233, dag_256_TRANSPOSE_SRC, dag_228_TC_SRC, dag_234}\n",
      "performing partition vectorization on AG_2[[0, 230, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_230_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 267, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_267_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 270, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_299_TRANSPOSE_SRC, dag_277, dag_271_TC_SRC, dag_270, dag_276}\n",
      "performing partition vectorization on AG_2[[0, 273, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_273_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 310, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_310_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 313, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_313, dag_319, dag_320, dag_314_TC_SRC, dag_342_TRANSPOSE_SRC}\n",
      "performing partition vectorization on AG_2[[0, 316, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_316_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 353, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_353_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 356, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_363, dag_357_TC_SRC, dag_385_TRANSPOSE_SRC, dag_356, dag_362}\n",
      "performing partition vectorization on AG_2[[0, 359, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_359_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 396, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_396_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 399, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_400_TC_SRC, dag_399, dag_428_TRANSPOSE_SRC, dag_405, dag_406}\n",
      "performing partition vectorization on AG_2[[0, 402, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_402_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 439, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_439_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 442, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_471_TRANSPOSE_SRC, dag_449, dag_443_TC_SRC, dag_442, dag_448}\n",
      "performing partition vectorization on AG_2[[0, 445, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_445_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 482, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_482_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 485, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_514_TRANSPOSE_SRC, dag_485, dag_486_TC_SRC, dag_491, dag_492}\n",
      "performing partition vectorization on AG_2[[0, 488, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_488_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 525, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_525_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 528, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_529_TC_SRC, dag_535, dag_557_TRANSPOSE_SRC, dag_528, dag_534}\n",
      "performing partition vectorization on AG_2[[0, 531, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_531_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 568, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_568_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 571, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_571, dag_577, dag_600_TRANSPOSE_SRC, dag_572_TC_SRC, dag_578}\n",
      "performing partition vectorization on AG_2[[0, 574, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_574_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 611, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_611_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 614, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_620, dag_615_TC_SRC, dag_621, dag_643_TRANSPOSE_SRC, dag_614}\n",
      "performing partition vectorization on AG_2[[0, 617, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_617_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 654, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_654_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 657, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_686_TRANSPOSE_SRC, dag_657, dag_663, dag_664, dag_658_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 660, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_660_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 697, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_697_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 700, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_706, dag_707, dag_701_TC_SRC, dag_729_TRANSPOSE_SRC, dag_700}\n",
      "performing partition vectorization on AG_2[[0, 703, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_703_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 740, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_740_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 743, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_744_TC_SRC, dag_750, dag_743, dag_749, dag_772_TRANSPOSE_SRC}\n",
      "performing partition vectorization on AG_2[[0, 746, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_746_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 783, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_783_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 786, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_786, dag_792, dag_815_TRANSPOSE_SRC, dag_793, dag_787_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 789, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_789_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 826, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_826_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 829, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_836, dag_858_TRANSPOSE_SRC, dag_830_TC_SRC, dag_829, dag_835}\n",
      "performing partition vectorization on AG_2[[0, 832, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_832_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 869, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_869_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 872, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_873_TC_SRC, dag_901_TRANSPOSE_SRC, dag_872, dag_878, dag_879}\n",
      "performing partition vectorization on AG_2[[0, 875, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_875_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 7, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_7_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 11, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_11_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 10, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_14_TC_SRC, dag_10}\n",
      "performing partition vectorization on AG_2[[0, 41, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_41_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 45, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_45_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 44, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_48_TC_SRC, dag_44}\n",
      "performing partition vectorization on AG_2[[0, 75, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_75_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 79, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_79_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 78, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_78, dag_82_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 109, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_109_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 113, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_113_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 112, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_112, dag_116_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 143, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_143_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 147, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_147_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 146, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_146, dag_150_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 177, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_177_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 181, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_181_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 180, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_180, dag_184_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 211, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_211_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 215, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_215_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 214, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_218_TC_SRC, dag_214}\n",
      "performing partition vectorization on AG_2[[0, 245, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_245_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 249, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_249_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 248, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_252_TC_SRC, dag_248}\n",
      "performing partition vectorization on AG_2[[0, 279, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_279_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 283, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_283_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 282, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_286_TC_SRC, dag_282}\n",
      "performing partition vectorization on AG_2[[0, 313, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_313_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 317, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_317_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 316, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_320_TC_SRC, dag_316}\n",
      "performing partition vectorization on AG_2[[0, 347, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_347_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 351, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_351_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 350, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_354_TC_SRC, dag_350}\n",
      "performing partition vectorization on AG_2[[0, 381, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_381_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 385, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_385_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 384, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_388_TC_SRC, dag_384}\n",
      "performing partition vectorization on AG_2[[0, 415, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_415_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 419, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_419_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 418, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_418, dag_422_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 449, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_449_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 453, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_453_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 452, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_452, dag_456_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 483, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_483_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 487, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_487_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 486, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_490_TC_SRC, dag_486}\n",
      "performing partition vectorization on AG_2[[0, 517, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_517_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 521, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_521_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 520, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_524_TC_SRC, dag_520}\n",
      "performing partition vectorization on AG_2[[0, 551, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_551_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 555, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_555_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 554, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_558_TC_SRC, dag_554}\n",
      "performing partition vectorization on AG_2[[0, 585, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_585_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 589, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_589_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 588, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_588, dag_592_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 619, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_619_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 623, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_623_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 622, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_626_TC_SRC, dag_622}\n",
      "performing partition vectorization on AG_2[[0, 653, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_653_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 657, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_657_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 656, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_660_TC_SRC, dag_656}\n",
      "performing partition vectorization on AG_2[[0, 687, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_687_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 691, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_691_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 690, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_694_TC_SRC, dag_690}\n",
      "performing partition vectorization on AG_2[[0, 721, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_721_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 7, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_7_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 11, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_11_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 10, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_10, dag_14_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 41, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_41_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 45, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_45_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 44, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_44, dag_48_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 75, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_75_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 79, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_79_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 78, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_78, dag_82_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 109, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_109_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 113, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_113_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 112, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_116_TC_SRC, dag_112}\n",
      "performing partition vectorization on AG_2[[0, 143, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_143_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 147, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_147_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 146, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_150_TC_SRC, dag_146}\n",
      "performing partition vectorization on AG_2[[0, 177, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_177_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 181, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_181_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 180, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_184_TC_SRC, dag_180}\n",
      "performing partition vectorization on AG_2[[0, 211, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_211_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 215, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_215_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 214, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_218_TC_SRC, dag_214}\n",
      "performing partition vectorization on AG_2[[0, 245, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_245_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 249, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_249_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 248, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_252_TC_SRC, dag_248}\n",
      "performing partition vectorization on AG_2[[0, 279, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_279_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 283, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_283_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 282, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_286_TC_SRC, dag_282}\n",
      "performing partition vectorization on AG_2[[0, 313, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_313_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 317, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_317_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 316, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_320_TC_SRC, dag_316}\n",
      "performing partition vectorization on AG_2[[0, 347, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_347_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 351, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_351_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 350, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_350, dag_354_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 381, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_381_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 385, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_385_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 384, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_388_TC_SRC, dag_384}\n",
      "performing partition vectorization on AG_2[[0, 415, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_415_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 419, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_419_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 418, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_422_TC_SRC, dag_418}\n",
      "performing partition vectorization on AG_2[[0, 449, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_449_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 453, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_453_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 452, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_452, dag_456_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 483, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_483_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 487, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_487_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 486, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_486, dag_490_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 517, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_517_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 521, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_521_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 520, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_520, dag_524_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 551, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_551_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 555, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_555_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 554, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_558_TC_SRC, dag_554}\n",
      "performing partition vectorization on AG_2[[0, 585, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_585_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 589, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_589_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 588, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_592_TC_SRC, dag_588}\n",
      "performing partition vectorization on AG_2[[0, 619, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_619_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 623, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_623_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 622, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_626_TC_SRC, dag_622}\n",
      "performing partition vectorization on AG_2[[0, 653, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_653_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 657, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_657_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 656, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_660_TC_SRC, dag_656}\n",
      "performing partition vectorization on AG_2[[0, 687, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_687_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 691, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_691_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 690, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_694_TC_SRC, dag_690}\n",
      "performing partition vectorization on AG_2[[0, 721, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_721_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 725, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_725_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 7, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_7_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 11, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_11_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 10, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_14_TC_SRC, dag_10}\n",
      "performing partition vectorization on AG_2[[0, 41, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_41_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 45, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_45_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 44, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_48_TC_SRC, dag_44}\n",
      "performing partition vectorization on AG_2[[0, 75, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_75_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 79, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_79_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 78, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_78, dag_82_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 109, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_109_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 113, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_113_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 112, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_112, dag_116_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 143, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_143_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 147, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_147_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 146, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_146, dag_150_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 177, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_177_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 181, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_181_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 180, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_180, dag_184_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 211, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_211_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 215, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_215_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 214, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_218_TC_SRC, dag_214}\n",
      "performing partition vectorization on AG_2[[0, 245, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_245_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 249, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_249_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 248, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_252_TC_SRC, dag_248}\n",
      "performing partition vectorization on AG_2[[0, 279, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_279_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 283, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_283_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 282, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_286_TC_SRC, dag_282}\n",
      "performing partition vectorization on AG_2[[0, 313, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_313_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 317, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_317_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 316, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_320_TC_SRC, dag_316}\n",
      "performing partition vectorization on AG_2[[0, 347, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_347_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 351, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_351_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 350, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_354_TC_SRC, dag_350}\n",
      "performing partition vectorization on AG_2[[0, 381, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_381_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 385, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_385_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 384, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_388_TC_SRC, dag_384}\n",
      "performing partition vectorization on AG_2[[0, 415, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_415_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 419, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_419_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 418, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_418, dag_422_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 449, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_449_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 453, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_453_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 452, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_452, dag_456_TC_SRC}\n",
      "performing partition vectorization on AG_2[[0, 483, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_483_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 487, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_487_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 486, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_490_TC_SRC, dag_486}\n",
      "performing partition vectorization on AG_2[[0, 517, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_517_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 521, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_521_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 520, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_524_TC_SRC, dag_520}\n",
      "performing partition vectorization on AG_2[[0, 551, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_551_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 555, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_555_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 554, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_558_TC_SRC, dag_554}\n",
      "performing partition vectorization on AG_2[[0, 585, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_585_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 589, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_589_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 588, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_592_TC_SRC, dag_588}\n",
      "performing partition vectorization on AG_2[[0, 619, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_619_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 623, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_623_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 622, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_626_TC_SRC, dag_622}\n",
      "performing partition vectorization on AG_2[[0, 653, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_653_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 657, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_657_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 656, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_660_TC_SRC, dag_656}\n",
      "performing partition vectorization on AG_2[[0, 687, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_687_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 691, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_691_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 690, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_694_TC_SRC, dag_690}\n",
      "performing partition vectorization on AG_2[[0, 721, 0, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_721_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 725, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_725_TC_DST}\n",
      "............2024-06-18 04:32:37.000398:  4220  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-06-18 04:32:37.000479:  4222  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "performing partition vectorization on AG_2[[0, 912, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_912_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 915, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_922, dag_916_TC_SRC, dag_944_TRANSPOSE_SRC, dag_915, dag_921}\n",
      "performing partition vectorization on AG_2[[0, 918, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_918_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 955, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_955_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 958, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_959_TC_SRC, dag_958, dag_964, dag_987_TRANSPOSE_SRC, dag_965}\n",
      "performing partition vectorization on AG_2[[0, 961, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_961_TC_DST}\n",
      "\n",
      "Compiler status PASS\n",
      "performing partition vectorization on AG_2[[0, 912, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_912_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 915, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_922, dag_916_TC_SRC, dag_944_TRANSPOSE_SRC, dag_915, dag_921}\n",
      "performing partition vectorization on AG_2[[0, 918, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_918_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 955, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_955_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 958, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_959_TC_SRC, dag_958, dag_964, dag_987_TRANSPOSE_SRC, dag_965}\n",
      "performing partition vectorization on AG_2[[0, 961, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_961_TC_DST}\n",
      "\n",
      "Compiler status PASS\n",
      "2024-06-18 04:32:38.000525:  4221  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "performing partition vectorization on AG_2[[0, 912, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_912_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 915, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_922, dag_916_TC_SRC, dag_944_TRANSPOSE_SRC, dag_915, dag_921}\n",
      "performing partition vectorization on AG_2[[0, 918, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_918_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 955, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_955_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 958, 0, 0]]{5 nodes (1 sources, 0 stops)}. dags covered: {dag_959_TC_SRC, dag_958, dag_964, dag_987_TRANSPOSE_SRC, dag_965}\n",
      "performing partition vectorization on AG_2[[0, 961, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_961_TC_DST}\n",
      "\n",
      "Compiler status PASS\n",
      "2024-06-18 04:32:39.000411:  4223  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "performing partition vectorization on AG_2[[0, 724, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_728_TC_SRC, dag_724}\n",
      "\n",
      "Compiler status PASS\n",
      "2024-06-18 04:32:41.000182:  4224  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "performing partition vectorization on AG_2[[0, 725, 0, 0]]{1 nodes (1 sources, 0 stops)}. dags covered: {dag_725_TC_DST}\n",
      "performing partition vectorization on AG_2[[0, 724, 0, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_728_TC_SRC, dag_724}\n",
      "\n",
      "Compiler status PASS\n",
      ".2024-06-18 04:32:51.000707:  4225  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "performing partition vectorization on AG_2[[0, 724, 0, 0, 0]]{2 nodes (1 sources, 0 stops)}. dags covered: {dag_728_TC_SRC, dag_724}\n",
      "\n",
      "Compiler status PASS\n",
      "2024-Jun-18 04:32:52.0366 3712:4217 [0] nccl_net_ofi_init:1415 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2024-Jun-18 04:32:52.0366 3712:4217 [0] init.cc:137 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n"
     ]
    }
   ],
   "source": [
    "from optimum.neuron import NeuronModelForCausalLM\n",
    "\n",
    "compiler_args = {\"num_cores\": 2, \"auto_cast_type\": 'fp16'}\n",
    "input_shapes = {\"batch_size\": 1, \"sequence_length\": 512}\n",
    "model = NeuronModelForCausalLM.from_pretrained(\n",
    "        \"TinyLlama/TinyLlama_v1.1\",\n",
    "        export=True,\n",
    "        **compiler_args,\n",
    "        **input_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a769ec67-8071-4601-ba47-0da4cc60cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"TinyLlama-neuron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63678cea-2bd8-4dfe-ac0e-22affd2bba7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<optimum.neuron.modeling.NeuronModelForCausalLM at 0x7fe057acbca0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.neuron import NeuronModelForCausalLM\n",
    "\n",
    "try:\n",
    "    model\n",
    "except NameError:\n",
    "    # Edit this to use another base model\n",
    "    model = NeuronModelForCausalLM.from_pretrained('TinyLlama-neuron')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57f2af25-0057-4d1e-8fe7-ff3f6892795d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5381d114363e4709b32e8e080cb01655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36178c037dd84fa68b85dcd8bb5c85c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227c2fb3fe924199b48bfc3fe591218f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2c8b3449bc4bec97005fa74b16778a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama_v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2ab553f-a92a-4fec-9131-78028b5f99a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Adjusting the maximum generation length (519) to the model maximum sequence length (512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What is deep-learning ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in the second part of the project ?\\nWhat is the deep learning model that we will build in']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"What is deep-learning ?\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs,\n",
    "                         max_new_tokens=512,\n",
    "                         do_sample=True,\n",
    "                         temperature=0.9,\n",
    "                         top_k=50,\n",
    "                         top_p=0.9)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f39847b6-4052-423a-bb82-1024679f0272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_prompt(message, history, max_tokens):\n",
    "    \"\"\" Convert a history of messages to a chat prompt\n",
    "\n",
    "\n",
    "    Args:\n",
    "        message(str): the new user message.\n",
    "        history (List[str]): the list of user messages and assistant responses.\n",
    "        max_tokens (int): the maximum number of input tokens accepted by the model.\n",
    "\n",
    "    Returns:\n",
    "        a `str` prompt.\n",
    "    \"\"\"\n",
    "    chat = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a friendly chatbot.\",\n",
    "        },\n",
    "    ]\n",
    "    # Convert all messages in history to chat interactions\n",
    "    for interaction in history:\n",
    "        chat.append({\"role\": \"user\", \"content\" : interaction[0]})\n",
    "        chat.append({\"role\": \"assistant\", \"content\" : interaction[1]})\n",
    "    # Add the new message\n",
    "    chat.append({\"role\": \"user\", \"content\" : message})\n",
    "    # Generate the prompt, verifying that we don't go beyond the maximum number of tokens\n",
    "    for i in range(0, len(chat), 2):\n",
    "        # Generate candidate prompt with the last n-i entries\n",
    "        prompt = tokenizer.apply_chat_template(chat[i:], tokenize=False)\n",
    "        # Tokenize to check if we're over the limit\n",
    "        tokens = tokenizer(prompt)\n",
    "        if len(tokens.input_ids) <= max_tokens:\n",
    "            # We're good, stop here\n",
    "            return prompt\n",
    "    # We shall never reach this line\n",
    "    raise SystemError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e05649b-f40e-40f4-988c-5672502dd511",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "max_tokens = 256\n",
    "\n",
    "def chat(message, history, max_tokens):\n",
    "    prompt = format_chat_prompt(message, history, max_tokens)\n",
    "    # Uncomment the line below to see what the formatted prompt looks like\n",
    "    #print(prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs,\n",
    "                             max_length=512,\n",
    "                             do_sample=True,\n",
    "                             temperature=0.9,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             repetition_penalty=1.2)\n",
    "    # Do not include the input tokens\n",
    "    outputs = outputs[0, inputs.input_ids.size(-1):]\n",
    "    response = tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "    history.append([message, response])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2064b34-b668-4b5a-8071-dbe1064cbef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I think that this is the Amazon Web Services. \n",
      "You have been asked to find out how many instances of a program, written in Go (the programming language created for Go), could be run on AWS in less than two minutes. The answer should not exceed 10 million (10^6).\n",
      "Go was designed as an open-source programming language with strong type checking and a high level of expressiveness. You can run it on most operating systems including: Linux, Mac OS X, and Windows. Go is free software, under the terms of GPL version 2 license.[/INST]  \n",
      "To understand how Amazon Web Service work you need go to http://aws.amazon.com/index.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chat(\"What is Amazon Web Services?\", history, max_tokens))\n",
    "# print(chat(\"Name a fruit that is on my favorite colour.\", history, max_tokens))\n",
    "# print(chat(\"What is the colour of my favorite fruit ?\", history, max_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
