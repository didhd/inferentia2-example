minWorkers: 1
maxWorkers: 1
maxBatchDelay: 100
responseTimeout: 10800
batchSize: 16

handler:
    model_path: "meta-llama/Meta-Llama-3-8B"
    model_checkpoint_dir: ""
    model_module_prefix: "transformers_neuronx"
    model_class_name: "llama.model.LlamaForSampling"
    # see https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/torch/transformers-neuronx/index.html#known-issues-and-limitations for llama2-13b
    # neuron_cc_flag: "-O1 --model-type=transformer --enable-mixed-precision-accumulation --enable-saturate-infinity"
    amp: "f16"
    tp_degree: 24
    max_length: 8192
    max_new_tokens: 50

micro_batching:
    micro_batch_size: 12
    parallelism:
        preprocess: 2
        inference: 1
        postprocess: 2