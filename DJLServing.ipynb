{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74156eb6-9517-475f-984e-2e76d24fb281",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy a fine-tuned TinyLlama-1.1B model for generative AI inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62083e2-b9dc-4f61-96f2-8a00056d19ce",
   "metadata": {},
   "source": [
    "## Specify the LMI container image\n",
    "\n",
    "[SageMaker LMI containers](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html) use [DJLServing](https://github.com/deepjavalibrary/djl-serving), a model server that is integrated with the [transformers-neuronx](https://github.com/aws-neuron/transformers-neuronx) library to support tensor parallelism across NeuronCores. The DJL model server and transformers-neuronx library serve as core components of the container, which also includes the [Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/). This setup facilitates the loading of models onto [AWS Inferentia2](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/inferentia2.html) accelerators, parallelizes the model across multiple [NeuronCores](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html#neuroncores-v2-arch), and enables serving via HTTP endpoints. This uses SageMaker Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04242641-befb-4e3e-a94f-51c2af7fbb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name SSMDefaultRoleForPVREReporting to get Role path.\n"
     ]
    }
   ],
   "source": [
    "import logging \n",
    "sagemaker_config_logger = logging.getLogger(\"sagemaker.config\") \n",
    "sagemaker_config_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Import SageMaker SDK, setup our session\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers\n",
    "import boto3\n",
    "\n",
    "# NOTE: We currently need to use us-east-2 for model deployment when running this notebook in an AWS Workshop Studio event.\n",
    "boto3_sess = boto3.Session(region_name=\"ap-northeast-1\")\n",
    "\n",
    "sess = sagemaker.session.Session(boto_session = boto3_sess)  # sagemaker session for interacting with different AWS APIs\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dff5e52d-2b6d-4e6e-b645-18c4174a8ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.ap-northeast-1.amazonaws.com/djl-inference:0.24.0-neuronx-sdk2.14.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-neuronx\",\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"0.24.0\"\n",
    "    )\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8fc7a-1608-4fa7-9799-34d4bbdcb09d",
   "metadata": {},
   "source": [
    "## Prepare Model Serving Artifacts\n",
    "\n",
    "The LMI container supports loading models from an Amazon Simple Storage Service (Amazon S3) bucket or Hugging Face Hub. You need  parameters required in *`serving.properties`* file to load and host the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a842f02a-3e2e-4d57-ab8e-472d9d89ea90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the serving.properties file required by the model server\n",
    "\n",
    "file_content = f\"\"\"engine=Python\n",
    "option.entryPoint=djl_python.transformers_neuronx\n",
    "option.model_id=TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "option.batch_size=1\n",
    "option.neuron_optimize_level=1\n",
    "option.tensor_parallel_degree=2\n",
    "option.load_in_8bit=false\n",
    "option.n_positions=512\n",
    "option.rolling_batch=auto\n",
    "option.dtype=fp16\"\"\"\n",
    "\n",
    "with open(\"serving.properties\",\"w\") as f:\n",
    "    f.write(file_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd30e5-fbd1-453a-b1c8-9b284357dca3",
   "metadata": {},
   "source": [
    "Construct the tarball containing *`serving.properties`* and upload it to an S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd069e9d-336b-4e11-8127-75a26210eeca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "cp serving.properties mycode/\n",
    "# tar czvf mycode.tar.gz mycode/\n",
    "# rm -rf mycode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df241f7-f8f3-4057-9c97-b8898371363d",
   "metadata": {},
   "source": [
    "## Create Container\n",
    "Next, we create the Container endpoint with the model configuration defined earlier. Model deployment will usually take 4-5 minutes as model is compiled during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ece310d1-3922-433d-a21d-9de242e29c57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!aws ecr get-login-password --region ap-northeast-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.ap-northeast-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df89a77b-8ca1-485d-8611-5d911c84b597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Starting model server ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mEc2Utils\u001b[m DJL will collect telemetry to help us better understand our users? needs, diagnose issues, and deliver additional features. If you would like to learn more or opt-out please go to: https://docs.djl.ai/docs/telemetry.html for more information.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Starting djl-serving: 0.24.0 ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m \n",
      "Model server home: /opt/djl\n",
      "Current directory: /opt/djl\n",
      "Temp directory: /tmp\n",
      "Command line: -Dlog4j.configurationFile=/usr/local/djl-serving-0.24.0/conf/log4j2.xml -Xmx1g -Xms1g -Xss2m -XX:+ExitOnOutOfMemoryError\n",
      "Number of CPUs: 192\n",
      "Number of Neuron cores: 24\n",
      "Max heap size: 1024\n",
      "Config file: /opt/djl/conf/config.properties\n",
      "Inference address: http://0.0.0.0:8080\n",
      "Management address: http://0.0.0.0:8080\n",
      "Default job_queue_size: 1000\n",
      "Default batch_size: 1\n",
      "Default max_batch_delay: 100\n",
      "Default max_idle_time: 60\n",
      "Model Store: /opt/ml/model\n",
      "Initial Models: ALL\n",
      "Netty threads: 0\n",
      "Maximum Request Size: 67108864\n",
      "Environment variables:\n",
      "\tDJL_CACHE_DIR: /tmp/.djl.ai\n",
      "\tOMP_NUM_THREADS: 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m scanning for plugins...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m scanning in plug-in folder :/opt/djl/plugins\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: cache-engines/jar:file:/opt/djl/plugins/cache-0.24.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: kserve/jar:file:/opt/djl/plugins/kserve-0.24.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: static-file-plugin/jar:file:/opt/djl/plugins/static-file-plugin-0.24.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: console/jar:file:/opt/djl/plugins/management-console-0.24.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {console/jar:file:/opt/djl/plugins/management-console-0.24.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin console changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {static-file-plugin/jar:file:/opt/djl/plugins/static-file-plugin-0.24.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin static-file-plugin changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {cache-engines/jar:file:/opt/djl/plugins/cache-0.24.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin cache-engines changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {kserve/jar:file:/opt/djl/plugins/kserve-0.24.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin kserve changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin console changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin static-file-plugin changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin cache-engines changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin kserve changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m 4 plug-ins found and loaded.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Found model model=file:/opt/ml/model/\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Initializing model: model=file:/opt/ml/model/\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Apply per model settings:\n",
      "\tjob_queue_size: 1000\n",
      "\tbatch_size: 1\n",
      "\tmax_batch_delay: 100\n",
      "\tmax_idle_time: 60\n",
      "\tload_on_devices: *\n",
      "\tengine: Python\n",
      "\toption.entryPoint: djl_python.transformers_neuronx\n",
      "\toption.batch_size: 1\n",
      "\toption.dtype: fp16\n",
      "\toption.load_in_8bit: false\n",
      "\toption.n_positions: 512\n",
      "\toption.tensor_parallel_degree: 2\n",
      "\toption.neuron_optimize_level: 1\n",
      "\toption.model_id: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "\toption.rolling_batch: auto\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPlatform\u001b[m Found matching platform from: jar:file:/usr/local/djl-serving-0.24.0/lib/python-0.24.0.jar!/native/lib/python.properties\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python_engine.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/arg_parser.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/deepspeed.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/encode_decode.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/fastertransformer.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/huggingface.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/inputs.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/np_util.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/outputs.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/pair_list.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/lmi_dist_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/neuron_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/scheduler_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/vllm_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/lm_block.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/search_config.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/seq_batch_scheduler.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/seq_batcher.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/seq_batcher_impl.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/step_generation.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/service_loader.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/stable-diffusion.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/stable_diffusion_inf2.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/streaming_utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/test_model.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/rolling_batch_test_scripts/efficiency_benchmark.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/rolling_batch_test_scripts/rolling_batch_scheduler.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/rolling_batch_test_scripts/run_rolling_batch_alone.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/rolling_batch_test_scripts/test_rolling_batch_scheduler2.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/rolling_batch_test_scripts/test_scheduler_sharded.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/test_input_output.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/test_scheduler.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/test_scheduler_bloom.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/optimum_neuron_scheduler.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/token_selector.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/ts_service_loader.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelManager\u001b[m Loading model on Python:[nc0, nc2, nc4, nc6, nc8, nc10, nc12, nc14, nc16, nc18, nc20, nc22]\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model model (PENDING) on nc(0) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 750088 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model model on nc(0)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19000 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=0-1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: 120 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '0']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: Python engine started.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: \n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading config.json: 100%|??????????| 608/608 [00:00<00:00, 194kB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: \n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading tokenizer_config.json: 100%|??????????| 1.29k/1.29k [00:00<00:00, 551kB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: \n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading tokenizer.model: 100%|??????????| 500k/500k [00:00<00:00, 245MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: \n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading tokenizer.json: 100%|??????????| 1.84M/1.84M [00:00<00:00, 2.80MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading tokenizer.json: 100%|??????????| 1.84M/1.84M [00:00<00:00, 2.79MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: \n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading (?)cial_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading (?)cial_tokens_map.json: 100%|??????????| 551/551 [00:00<00:00, 1.31MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: \n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:   2%|?         | 41.9M/2.20G [00:00<00:05, 363MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:   4%|?         | 83.9M/2.20G [00:00<00:05, 370MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:   6%|?         | 126M/2.20G [00:00<00:05, 371MB/s] \n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:   8%|?         | 168M/2.20G [00:00<00:05, 370MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  10%|?         | 210M/2.20G [00:00<00:06, 299MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  12%|??        | 262M/2.20G [00:00<00:05, 340MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  14%|??        | 304M/2.20G [00:00<00:05, 322MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  16%|??        | 346M/2.20G [00:01<00:05, 338MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  18%|??        | 388M/2.20G [00:01<00:06, 296MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  19%|??        | 419M/2.20G [00:01<00:06, 276MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  20%|??        | 451M/2.20G [00:01<00:07, 230MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  22%|???       | 482M/2.20G [00:01<00:07, 225MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  24%|???       | 524M/2.20G [00:01<00:06, 251MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  26%|???       | 566M/2.20G [00:01<00:05, 287MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  27%|???       | 598M/2.20G [00:02<00:05, 290MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  29%|???       | 640M/2.20G [00:02<00:04, 314MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  31%|???       | 682M/2.20G [00:02<00:04, 332MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  33%|????      | 724M/2.20G [00:02<00:04, 350MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  35%|????      | 776M/2.20G [00:02<00:03, 390MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  38%|????      | 828M/2.20G [00:02<00:03, 403MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  40%|????      | 881M/2.20G [00:02<00:03, 431MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  42%|?????     | 933M/2.20G [00:02<00:02, 427MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  45%|?????     | 986M/2.20G [00:02<00:02, 431MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  47%|?????     | 1.04G/2.20G [00:03<00:02, 419MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  50%|?????     | 1.09G/2.20G [00:03<00:02, 416MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  51%|??????    | 1.13G/2.20G [00:03<00:02, 414MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  54%|??????    | 1.18G/2.20G [00:03<00:02, 429MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  56%|??????    | 1.24G/2.20G [00:03<00:02, 442MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  59%|??????    | 1.29G/2.20G [00:03<00:02, 447MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  61%|??????    | 1.34G/2.20G [00:03<00:01, 447MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  63%|???????   | 1.39G/2.20G [00:03<00:01, 412MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  65%|???????   | 1.44G/2.20G [00:04<00:01, 405MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  67%|???????   | 1.48G/2.20G [00:04<00:01, 405MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  70%|???????   | 1.53G/2.20G [00:04<00:01, 420MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  72%|????????  | 1.58G/2.20G [00:04<00:01, 405MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  74%|????????  | 1.63G/2.20G [00:04<00:01, 373MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  76%|????????  | 1.67G/2.20G [00:04<00:01, 381MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  78%|????????  | 1.72G/2.20G [00:04<00:01, 407MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  81%|????????  | 1.77G/2.20G [00:04<00:00, 432MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  83%|????????? | 1.82G/2.20G [00:04<00:00, 395MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  85%|????????? | 1.87G/2.20G [00:05<00:00, 377MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  87%|????????? | 1.91G/2.20G [00:05<00:00, 385MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  89%|????????? | 1.95G/2.20G [00:05<00:00, 371MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  91%|????????? | 1.99G/2.20G [00:05<00:00, 357MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  93%|??????????| 2.04G/2.20G [00:05<00:00, 392MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  95%|??????????| 2.09G/2.20G [00:05<00:00, 398MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors:  97%|??????????| 2.14G/2.20G [00:05<00:00, 394MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors: 100%|??????????| 2.19G/2.20G [00:05<00:00, 400MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading model.safetensors: 100%|??????????| 2.20G/2.20G [00:05<00:00, 369MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: \n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stderr: Downloading generation_config.json: 100%|??????????| 124/124 [00:00<00:00, 50.5kB/s]\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: 2024-06-18 13:48:09.000030:  134  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: 2024-06-18 13:48:09.000032:  134  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/no-user/neuroncc_compile_workdir/98210d5a-5993-4351-8387-51c57f82364f/model.MODULE_a5e62e3c494cc4d4bc8e+016d3e7a.hlo.pb', '--output', '/tmp/no-user/neuroncc_compile_workdir/98210d5a-5993-4351-8387-51c57f82364f/model.MODULE_a5e62e3c494cc4d4bc8e+016d3e7a.neff', '--logfile', '/tmp/compile.log', '--temp-dir=/tmp', '--model-type=transformer', '-O1', '--model-type=transformer', '--verbose=35']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: 2024-06-18 13:48:09.000056:  135  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: 2024-06-18 13:48:09.000058:  135  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/no-user/neuroncc_compile_workdir/0bbd25bb-a9db-4a12-9d16-81c799bb5808/model.MODULE_e7507bf770860b681403+016d3e7a.hlo.pb', '--output', '/tmp/no-user/neuroncc_compile_workdir/0bbd25bb-a9db-4a12-9d16-81c799bb5808/model.MODULE_e7507bf770860b681403+016d3e7a.neff', '--logfile', '/tmp/compile.log', '--temp-dir=/tmp', '--model-type=transformer', '-O1', '--model-type=transformer', '--verbose=35']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: 2024-06-18 13:48:09.000081:  136  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: 2024-06-18 13:48:09.000083:  136  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/no-user/neuroncc_compile_workdir/6209a405-2d22-441f-8d9d-6a6540d2e5f1/model.MODULE_8972fee769e3352ff5a3+016d3e7a.hlo.pb', '--output', '/tmp/no-user/neuroncc_compile_workdir/6209a405-2d22-441f-8d9d-6a6540d2e5f1/model.MODULE_8972fee769e3352ff5a3+016d3e7a.neff', '--logfile', '/tmp/compile.log', '--temp-dir=/tmp', '--model-type=transformer', '-O1', '--model-type=transformer', '--verbose=35']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: ......\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: Compiler status PASS\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: \n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: Compiler status PASS\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: \n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: Compiler status PASS\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: 2024-06-18 13:48:33.000593:  790  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: 2024-06-18 13:48:33.000595:  790  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/no-user/neuroncc_compile_workdir/3bd785df-4644-482a-a9a3-f2c1bdcbbb39/model.MODULE_b91b69273f8cd1de8fcf+016d3e7a.hlo.pb', '--output', '/tmp/no-user/neuroncc_compile_workdir/3bd785df-4644-482a-a9a3-f2c1bdcbbb39/model.MODULE_b91b69273f8cd1de8fcf+016d3e7a.neff', '--logfile', '/tmp/compile.log', '--temp-dir=/tmp', '--model-type=transformer', '-O1', '--model-type=transformer', '--verbose=35']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: .\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: Compiler status PASS\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: 2024-06-18 13:48:42.000127:  1009  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: 2024-06-18 13:48:42.000129:  1009  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/no-user/neuroncc_compile_workdir/dd2140b4-6d83-4a3d-b6cd-dd2c1c6c7d78/model.MODULE_869cd543241d0405b9eb+016d3e7a.hlo.pb', '--output', '/tmp/no-user/neuroncc_compile_workdir/dd2140b4-6d83-4a3d-b6cd-dd2c1c6c7d78/model.MODULE_869cd543241d0405b9eb+016d3e7a.neff', '--logfile', '/tmp/compile.log', '--temp-dir=/tmp', '--model-type=transformer', '-O1', '--model-type=transformer', '--verbose=35']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: .\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: Compiler status PASS\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: 2024-06-18 13:48:51.000175:  1228  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: 2024-06-18 13:48:51.000177:  1228  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/no-user/neuroncc_compile_workdir/11eb8052-c75b-48ed-95f1-b38392d72da8/model.MODULE_b3c4cedbdc8ed79e1006+016d3e7a.hlo.pb', '--output', '/tmp/no-user/neuroncc_compile_workdir/11eb8052-c75b-48ed-95f1-b38392d72da8/model.MODULE_b3c4cedbdc8ed79e1006+016d3e7a.neff', '--logfile', '/tmp/compile.log', '--temp-dir=/tmp', '--model-type=transformer', '-O1', '--model-type=transformer', '--verbose=35']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: .\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: Compiler status PASS\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: SysHealth: LLM sharding and compilation latency: 62.14894461631775 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [model] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model model (READY) on nc(2) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 744609 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model model on nc(2)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19001 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=2-3\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: 1449 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19001', '--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '2']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: 2024-06-18 13:49:16.000342:  1464  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: 2024-06-18 13:49:16.000348:  1464  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: 2024-06-18 13:49:16.000373:  1465  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: 2024-06-18 13:49:16.000385:  1465  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: 2024-06-18 13:49:16.000397:  1466  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: 2024-06-18 13:49:16.000404:  1466  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: 2024-06-18 13:49:17.000480:  1475  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: 2024-06-18 13:49:17.000490:  1475  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: 2024-06-18 13:49:18.000525:  1480  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: 2024-06-18 13:49:18.000536:  1480  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: 2024-06-18 13:49:19.000584:  1485  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: 2024-06-18 13:49:19.000597:  1485  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: SysHealth: LLM sharding and compilation latency: 14.254701137542725 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [model] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model model (READY) on nc(4) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 739150 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model model on nc(4)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19002 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=4-5\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: 1492 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19002', '--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '4']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: 2024-06-18 13:49:35.000811:  1504  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: 2024-06-18 13:49:35.000818:  1504  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: 2024-06-18 13:49:35.000838:  1505  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: 2024-06-18 13:49:35.000845:  1505  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: 2024-06-18 13:49:35.000866:  1506  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: 2024-06-18 13:49:35.000872:  1506  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: 2024-06-18 13:49:36.000942:  1515  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: 2024-06-18 13:49:36.000952:  1515  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: 2024-06-18 13:49:37.000919:  1520  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: 2024-06-18 13:49:37.000930:  1520  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: 2024-06-18 13:49:39.000003:  1525  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: 2024-06-18 13:49:39.000016:  1525  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: SysHealth: LLM sharding and compilation latency: 14.340785026550293 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [model] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model model (READY) on nc(6) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 734090 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model model on nc(6)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19003 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=6-7\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: 1532 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19003', '--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '6']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: 2024-06-18 13:49:56.000026:  1545  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: 2024-06-18 13:49:56.000033:  1545  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: 2024-06-18 13:49:56.000053:  1546  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: 2024-06-18 13:49:56.000060:  1546  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: 2024-06-18 13:49:56.000080:  1547  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: 2024-06-18 13:49:56.000086:  1547  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: 2024-06-18 13:49:57.000139:  1556  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: 2024-06-18 13:49:57.000149:  1556  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: 2024-06-18 13:49:58.000187:  1561  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: 2024-06-18 13:49:58.000198:  1561  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: 2024-06-18 13:49:59.000268:  1566  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: 2024-06-18 13:49:59.000281:  1566  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1532-model-stdout: SysHealth: LLM sharding and compilation latency: 14.411816596984863 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [model] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model model (READY) on nc(8) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 728629 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model model on nc(8)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19004 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=8-9\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: 1573 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19004', '--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '8']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: 2024-06-18 13:50:15.000755:  1585  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: 2024-06-18 13:50:15.000762:  1585  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: 2024-06-18 13:50:15.000781:  1586  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: 2024-06-18 13:50:15.000788:  1586  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: 2024-06-18 13:50:15.000807:  1587  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: 2024-06-18 13:50:15.000814:  1587  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: 2024-06-18 13:50:16.000909:  1596  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: 2024-06-18 13:50:16.000919:  1596  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: 2024-06-18 13:50:17.000994:  1601  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: 2024-06-18 13:50:18.000004:  1601  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: 2024-06-18 13:50:19.000092:  1606  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: 2024-06-18 13:50:19.000104:  1606  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1573-model-stdout: SysHealth: LLM sharding and compilation latency: 14.372694253921509 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [model] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model model (READY) on nc(10) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 723721 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model model on nc(10)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19005 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=10-11\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: 1613 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19005', '--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '10']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: 2024-06-18 13:50:35.000957:  1625  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: 2024-06-18 13:50:35.000964:  1625  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: 2024-06-18 13:50:35.000993:  1626  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: 2024-06-18 13:50:36.000006:  1626  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: 2024-06-18 13:50:36.000023:  1627  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: 2024-06-18 13:50:36.000031:  1627  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: 2024-06-18 13:50:37.000090:  1636  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: 2024-06-18 13:50:37.000100:  1636  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: 2024-06-18 13:50:38.000075:  1641  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: 2024-06-18 13:50:38.000085:  1641  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: 2024-06-18 13:50:39.000069:  1646  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: 2024-06-18 13:50:39.000082:  1646  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1613-model-stdout: SysHealth: LLM sharding and compilation latency: 14.166260242462158 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [model] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model model (READY) on nc(12) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 718637 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model model on nc(12)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19006 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=12-13\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: 1654 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19006', '--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '12']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: 2024-06-18 13:50:55.000426:  1666  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: 2024-06-18 13:50:55.000433:  1666  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: 2024-06-18 13:50:55.000454:  1667  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: 2024-06-18 13:50:55.000461:  1667  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: 2024-06-18 13:50:55.000481:  1668  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: 2024-06-18 13:50:55.000488:  1668  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: 2024-06-18 13:50:56.000567:  1677  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: 2024-06-18 13:50:56.000577:  1677  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: 2024-06-18 13:50:57.000635:  1682  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: 2024-06-18 13:50:57.000645:  1682  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: 2024-06-18 13:50:58.000719:  1687  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: 2024-06-18 13:50:58.000732:  1687  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1654-model-stdout: SysHealth: LLM sharding and compilation latency: 14.450517654418945 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [model] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model model (READY) on nc(14) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 713541 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model model on nc(14)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19007 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=14-15\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: 1695 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19007', '--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '14']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: 2024-06-18 13:51:15.000911:  1707  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: 2024-06-18 13:51:15.000918:  1707  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: 2024-06-18 13:51:15.000938:  1708  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: 2024-06-18 13:51:15.000945:  1708  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: 2024-06-18 13:51:15.000963:  1709  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: 2024-06-18 13:51:15.000970:  1709  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: 2024-06-18 13:51:17.000029:  1718  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: 2024-06-18 13:51:17.000039:  1718  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: 2024-06-18 13:51:18.000069:  1723  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: 2024-06-18 13:51:18.000080:  1723  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: 2024-06-18 13:51:19.000173:  1729  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: 2024-06-18 13:51:19.000186:  1729  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1695-model-stdout: SysHealth: LLM sharding and compilation latency: 14.310870170593262 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [model] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model model (READY) on nc(16) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 708069 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model model on nc(16)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19008 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=16-17\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: 1736 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19008', '--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '16']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: 2024-06-18 13:51:35.000366:  1758  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: 2024-06-18 13:51:35.000373:  1758  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: 2024-06-18 13:51:35.000393:  1759  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: 2024-06-18 13:51:35.000400:  1759  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: 2024-06-18 13:51:35.000419:  1760  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: 2024-06-18 13:51:35.000426:  1760  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: 2024-06-18 13:51:36.000507:  1769  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: 2024-06-18 13:51:36.000517:  1769  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: 2024-06-18 13:51:37.000584:  1774  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: 2024-06-18 13:51:37.000595:  1774  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: 2024-06-18 13:51:38.000596:  1779  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: 2024-06-18 13:51:38.000608:  1779  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1736-model-stdout: SysHealth: LLM sharding and compilation latency: 14.291587591171265 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [model] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model model (READY) on nc(18) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 702592 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model model on nc(18)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19009 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=18-19\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: 1786 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19009', '--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '18']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: 2024-06-18 13:51:54.000822:  1799  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: 2024-06-18 13:51:54.000829:  1799  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: 2024-06-18 13:51:54.000849:  1800  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: 2024-06-18 13:51:54.000856:  1800  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: 2024-06-18 13:51:54.000876:  1801  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: 2024-06-18 13:51:54.000882:  1801  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: 2024-06-18 13:51:55.000961:  1810  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: 2024-06-18 13:51:55.000971:  1810  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: 2024-06-18 13:51:57.000030:  1815  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: 2024-06-18 13:51:57.000040:  1815  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: 2024-06-18 13:51:58.000091:  1820  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: 2024-06-18 13:51:58.000104:  1820  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1786-model-stdout: SysHealth: LLM sharding and compilation latency: 14.31958293914795 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [model] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model model (READY) on nc(20) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 697695 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model model on nc(20)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19010 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=20-21\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: 1827 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19010', '--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '20']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: 2024-06-18 13:52:14.000277:  1839  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: 2024-06-18 13:52:14.000284:  1839  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: 2024-06-18 13:52:14.000304:  1840  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: 2024-06-18 13:52:14.000310:  1840  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: 2024-06-18 13:52:14.000332:  1841  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: 2024-06-18 13:52:14.000339:  1841  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: 2024-06-18 13:52:15.000400:  1850  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: 2024-06-18 13:52:15.000410:  1850  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: 2024-06-18 13:52:16.000373:  1855  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: 2024-06-18 13:52:16.000383:  1855  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: 2024-06-18 13:52:17.000369:  1860  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: 2024-06-18 13:52:17.000382:  1860  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1827-model-stdout: SysHealth: LLM sharding and compilation latency: 14.200259923934937 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [model] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model model (READY) on nc(22) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 692236 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model model on nc(22)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19011 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=22-23\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: 1867 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19011', '--model-dir', '/opt/ml/model', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '22']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: 2024-06-18 13:52:33.000760:  1879  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: 2024-06-18 13:52:33.000767:  1879  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: 2024-06-18 13:52:33.000787:  1880  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: 2024-06-18 13:52:33.000794:  1880  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: 2024-06-18 13:52:33.000813:  1881  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: 2024-06-18 13:52:33.000820:  1881  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: 2024-06-18 13:52:34.000878:  1890  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: 2024-06-18 13:52:34.000888:  1890  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: 2024-06-18 13:52:35.000949:  1895  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: 2024-06-18 13:52:35.000960:  1895  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: 2024-06-18 13:52:37.000028:  1900  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: 2024-06-18 13:52:37.000042:  1900  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1867-model-stdout: SysHealth: LLM sharding and compilation latency: 14.453749179840088 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [model] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Initialize BOTH server with: EpollServerSocketChannel.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m BOTH API bind to: http://0.0.0.0:8080\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: Rolling batch inference error\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: Traceback (most recent call last):\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout:   File \"/tmp/.djl.ai/python/0.24.0/djl_python/rolling_batch/rolling_batch.py\", line 114, in try_catch_handling\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout:     return func(self, input_data, parameters)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout:   File \"/tmp/.djl.ai/python/0.24.0/djl_python/rolling_batch/neuron_rolling_batch.py\", line 40, in inference\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout:     generations = self.scheduler.prefill(new_requests)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout:   File \"/tmp/.djl.ai/python/0.24.0/djl_python/transformers_neuronx_scheduler/optimum_neuron_scheduler.py\", line 225, in prefill\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout:     padded_inputs = self.tokenizer(inputs,\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout:   File \"/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\", line 2806, in __call__\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout:     encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout:   File \"/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\", line 2864, in _call_one\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout:     raise ValueError(\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-model-stdout: ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1449-model-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-model-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "!docker run -it --rm  --rm --network=host \\\n",
    "  -v $(pwd)/mycode:/opt/ml/model/ \\\n",
    "  -v `pwd`/logs:/opt/djl/logs \\\n",
    "  -u djl \\\n",
    "  --device /dev/neuron0 \\\n",
    "  --device /dev/neuron1 \\\n",
    "  --device /dev/neuron2 \\\n",
    "  --device /dev/neuron3 \\\n",
    "  --device /dev/neuron4 \\\n",
    "  --device /dev/neuron5 \\\n",
    "  --device /dev/neuron6 \\\n",
    "  --device /dev/neuron7 \\\n",
    "  --device /dev/neuron8 \\\n",
    "  --device /dev/neuron9 \\\n",
    "  --device /dev/neuron10 \\\n",
    "  --device /dev/neuron11 \\\n",
    "  -e MODEL_LOADING_TIMEOUT=7200 \\\n",
    "  -e PREDICT_TIMEOUT=360 \\\n",
    "  {image_uri} serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a61a226-3178-45a6-a2fb-9f7bec510370",
   "metadata": {},
   "source": [
    "## Inference tests\n",
    "After the SageMaker endpoint has been created, we can make real-time predictions against SageMaker endpoints using the Predictor object:\n",
    "- Create a predictor for submit inference requests and receive reponses\n",
    "- Requests and responses are in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cac86-9b46-4d78-a914-2d0c82f58ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "curl -X POST \"http://127.0.0.1:8080/predictions/model\" \\\n",
    "     -H 'Content-Type: application/json' \\\n",
    "     -d '{\"seq_length\":512,\n",
    "          \"inputs\":\n",
    "                    \"Welcome to Amazon Elastic Compute Cloud,\"\n",
    "          }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4977b17d-dabc-45ab-89e9-43621f9ac7fe",
   "metadata": {},
   "source": [
    "Lets submit an inference requests to model server and receive inference result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfdec03-db06-4a78-9d18-b1a5b0bfe752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review_text = \"I couldn't believe this was the same director as Antonia's Line.<br /><br />This film has it all, \\\n",
    "a boring plot, disjointed flashbacks, a subplot that has nothing to do with the main plot what so ever, \\\n",
    "and totally uninteresting characters.It was painful to watch. Soooo, painful.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30696aa-7472-41bd-8da8-eb17f1aa9253",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"###Query: Classify the following movie review as positive or negative\\n \\\n",
    "###Review: {review_text}\\n \\\n",
    "###Classification:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3cc05a-167c-43e8-9316-7d28383bb604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = predictor.predict(\n",
    "    {\"inputs\": prompt, \"parameters\": {\"max_new_tokens\":32, \"do_sample\":\"true\"}}\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b81c30-818e-4566-ad42-68f9e75ca228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review_text = \"This movie is one of my all-time favorites. I think that Sean Penn did a great job acting. \\\n",
    "It is one of the few true stories that made it to film that I really like. It is in my top 10 films of all-time. \\\n",
    "I watch it over and over and never get tired of it. Great movie!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26540db1-d60d-42c1-9c90-e2ea0d39445e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"###Query: Classify the following movie review as positive or negative\\n \\\n",
    "###Review: {review_text}\\n \\\n",
    "###Classification:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c13cb9-6d20-4e48-a14c-25717a1a307c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = predictor.predict(\n",
    "    {\"inputs\": prompt, \"parameters\": {\"max_new_tokens\":32, \"do_sample\":\"true\"}}\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37c4b5-8c22-42e8-8509-00c520b7cc8f",
   "metadata": {},
   "source": [
    "## Cleanup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db708703-2952-405f-8e47-c026cc055448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad8af2-1289-4b6c-ae4d-59eafa0eeb50",
   "metadata": {
    "tags": []
   },
   "source": [
    "Congratulations on completing the LLM deployment for the inference module!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d3363-d20c-4911-83dd-8d1e619024e8",
   "metadata": {},
   "source": [
    "## (Optional) Deploy original TinyLlama model from Hugging Face hub\n",
    "\n",
    "If you have spare time, you can also consider an optional step of deploying the original TinyLlama model from [Hugging Face hub](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.4) for even more fun !\n",
    "\n",
    "In this scenario, you can specify the name of the Hugging Face model using the *`model_id`* parameter to download the model directly from the Hugging Face repo. The remaining steps of the process remain the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd14a7-e172-48fb-987d-5100f1a08f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-neuronx\",\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"0.24.0\"\n",
    "    )\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec381ba-6a96-4b9e-9ee0-bdcca0fe12e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "engine=Python\n",
    "option.entryPoint=djl_python.transformers_neuronx\n",
    "option.model_id=TinyLlama/TinyLlama-1.1B-Chat-v0.4\n",
    "option.batch_size=1\n",
    "option.neuron_optimize_level=1\n",
    "option.tensor_parallel_degree=2\n",
    "option.load_in_8bit=false\n",
    "option.n_positions=512\n",
    "option.rolling_batch=auto\n",
    "option.dtype=fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3517f086-9ed2-47a5-b682-5b8685dc14f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir mycode\n",
    "mv serving.properties mycode/\n",
    "tar czvf mycode.tar.gz mycode/\n",
    "rm -rf mycode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bbfc6e-67f0-425e-bb49-36953ae4d191",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_prefix = \"neuron_events2024/large-model-lmi/code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mycode.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"Code uploaded to --- > {code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fedb23-8016-4867-b76b-f23f6a831800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.inf2.xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"tinyllama-original-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc12867b-4757-4261-a9f3-dde215c1d64d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role, sagemaker_session = sess)\n",
    "\n",
    "model._is_compiled_model = True\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             container_startup_health_check_timeout=500,\n",
    "             volume_size=256,\n",
    "             endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b7f4fc-2d7d-4440-b2d0-9644a9065338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f85d5a-1df9-49bb-969a-6959c2faf37b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"How to get in a good university?\"\n",
    "formatted_prompt = (\n",
    "    f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e958823-a660-4ab4-a377-edefdd31bfa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = predictor.predict(\n",
    "    {\"inputs\": formatted_prompt, \"parameters\": {\"max_new_tokens\":512, \"do_sample\":\"true\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d51a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.loads(result)[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed84cad1-45c6-4b97-8401-5c5f63564bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
