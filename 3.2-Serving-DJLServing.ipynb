{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74156eb6-9517-475f-984e-2e76d24fb281",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 실시간 엔드포인트를 DJL Serving을 이용해 호스팅\n",
    "이번 노트북에서는 [Deep Java Library](https://djl.ai/) (DJServing)가 지원하는 대형 모델 추론 컨테이너(LMI)를 모델 서빙 솔루션으로 사용합니다. \n",
    "\n",
    "AWS Neuron SDK는 사용자가 Inferentia 장치의 강력한 처리 능력을 쉽게 활용할 수 있게 해주며, DJLServing은 Java 기반 환경에서 대규모 모델을 손쉽게 서빙할 수 있도록 지원합니다. 이 노트북은 Amazon Elastic Compute Cloud(Amazon EC2) inf2.xlarge 인스턴스에 TinyLLama 모델을 배포합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62083e2-b9dc-4f61-96f2-8a00056d19ce",
   "metadata": {},
   "source": [
    "## 컨테이너 이미지 확인\n",
    "\n",
    "LMI 컨테이너는 [DJLServing](https://github.com/deepjavalibrary/djl-serving)을 사용합니다. DJLServing은 [transformers-neuronx](https://github.com/aws-neuron/transformers-neuronx) 라이브러리와 통합되어 있어, NeuronCores 간의 텐서 병렬 처리를 지원하는 모델 서버입니다. DJL 모델 서버와 transformers-neuronx 라이브러리는 컨테이너의 핵심 구성 요소로 작용하며, [Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/)도 포함되어 있습니다. 이 설정은 모델을 [AWS Inferentia2](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/inferentia2.html) 가속기에 로드하고, 여러 [NeuronCores](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html#neuroncores-v2-arch)에 걸쳐 모델을 병렬 처리하며, HTTP 엔드포인트를 통해 서비스를 제공하는 데 도움이 됩니다. 이는 SageMaker 라이브러리를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff5e52d-2b6d-4e6e-b645-18c4174a8ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.ap-northeast-1.amazonaws.com/djl-inference:0.24.0-neuronx-sdk2.14.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: 다음 코드를 통해 이미지 주소를 가져올 수 있지만, 여기선 건너뜁니다.\n",
    "\n",
    "# import sagemaker\n",
    "# from sagemaker import Model, image_uris, serializers\n",
    "# import boto3\n",
    "\n",
    "# boto3_sess = boto3.Session(region_name=\"ap-northeast-1\")\n",
    "\n",
    "# sess = sagemaker.session.Session(boto_session = boto3_sess)  # sagemaker session for interacting with different AWS APIs\n",
    "# role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "\n",
    "# image_uri = image_uris.retrieve(\n",
    "#         framework=\"djl-neuronx\",\n",
    "#         region=sess.boto_session.region_name,\n",
    "#         version=\"0.24.0\"\n",
    "#     )\n",
    "\n",
    "image_uri = \"763104351884.dkr.ecr.ap-northeast-1.amazonaws.com/djl-inference:0.24.0-neuronx-sdk2.14.1\"\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8fc7a-1608-4fa7-9799-34d4bbdcb09d",
   "metadata": {},
   "source": [
    "## 모델 서빙 아티팩트 준비하기\n",
    "\n",
    "LMI 컨테이너는 Amazon Simple Storage Service (Amazon S3) 버킷 또는 Hugging Face Hub에서 모델을 로드하는 것을 지원합니다. 이때는 S3 또는 HuggingFace의 SafeTensor 형식의 모델이, 컨테이너 내에서 시작 전 자동으로 컴파일 됩니다. 모델을 로드하고 호스팅하기 위해 *`serving.properties`* 파일에 필요한 파라미터들이 필요합니다. 모델 로딩 시 사용될 수 있도록 serving.properties 파일에 구성 가능한 파라미터를 제공할 수 있습니다. 구성 가능한 파라미터의 전체 목록은 [All DJL configuration options](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/configurations.md)에서 참조하세요. (참고: 직접 model.py 파일을 작성하고 서빙 코드에 사용하도록 할 수도 있지만, 이 경우 DJLServing API와 transformers-neuronx API 간의 연결 다리 역할을 할 모델 로딩 및 추론 메소드를 구현해야 합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a842f02a-3e2e-4d57-ab8e-472d9d89ea90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모델 서버에 필요한 serving.properties 파일 생성 (Download from HuggingFace or S3)\n",
    "\n",
    "file_content = f\"\"\"engine=Python\n",
    "option.entryPoint=djl_python.transformers_neuronx\n",
    "option.model_id=TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "option.batch_size=1\n",
    "option.neuron_optimize_level=1\n",
    "option.tensor_parallel_degree=2\n",
    "option.load_in_8bit=false\n",
    "option.n_positions=512\n",
    "option.rolling_batch=auto\n",
    "option.dtype=fp16\"\"\"\n",
    "\n",
    "with open(\"serving.properties\",\"w\") as f:\n",
    "    f.write(file_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502450fe-e7a5-4148-9554-d3a39480050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# `serving.properties`를 models/tinyllama 디렉토리로 이동\n",
    "rm -rf models \n",
    "rm -rf logs\n",
    "mkdir -p models/tinyllama logs\n",
    "cp serving.properties models/tinyllama/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df241f7-f8f3-4057-9c97-b8898371363d",
   "metadata": {},
   "source": [
    "## 컨테이너 생성\n",
    "다음으로, 앞서 정의한 모델 설정을 사용하여 컨테이너 엔드포인트를 생성합니다. 모델 배포는 보통 4-5분 정도 소요되며, 이 과정에서 모델이 컴파일됩니다. (HuggingFace 또는 S3를 model_id로 지정한 경우)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece310d1-3922-433d-a21d-9de242e29c57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!aws ecr get-login-password --region ap-northeast-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.ap-northeast-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df89a77b-8ca1-485d-8611-5d911c84b597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Starting model server ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mEc2Utils\u001b[m DJL will collect telemetry to help us better understand our users? needs, diagnose issues, and deliver additional features. If you would like to learn more or opt-out please go to: https://docs.djl.ai/docs/telemetry.html for more information.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Starting djl-serving: 0.24.0 ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m \n",
      "Model server home: /opt/djl\n",
      "Current directory: /opt/djl\n",
      "Temp directory: /tmp\n",
      "Command line: -Dlog4j.configurationFile=/usr/local/djl-serving-0.24.0/conf/log4j2.xml -Xmx1g -Xms1g -Xss2m -XX:+ExitOnOutOfMemoryError\n",
      "Number of CPUs: 192\n",
      "Number of Neuron cores: 24\n",
      "Max heap size: 1024\n",
      "Config file: /opt/djl/conf/config.properties\n",
      "Inference address: http://0.0.0.0:8080\n",
      "Management address: http://0.0.0.0:8080\n",
      "Default job_queue_size: 1000\n",
      "Default batch_size: 1\n",
      "Default max_batch_delay: 100\n",
      "Default max_idle_time: 60\n",
      "Model Store: /opt/ml/model\n",
      "Initial Models: ALL\n",
      "Netty threads: 0\n",
      "Maximum Request Size: 67108864\n",
      "Environment variables:\n",
      "\tDJL_CACHE_DIR: /tmp/.djl.ai\n",
      "\tOMP_NUM_THREADS: 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m scanning for plugins...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m scanning in plug-in folder :/opt/djl/plugins\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: cache-engines/jar:file:/opt/djl/plugins/cache-0.24.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: kserve/jar:file:/opt/djl/plugins/kserve-0.24.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: static-file-plugin/jar:file:/opt/djl/plugins/static-file-plugin-0.24.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPropertyFilePluginMetaDataReader\u001b[m Plugin found: console/jar:file:/opt/djl/plugins/management-console-0.24.0.jar!/META-INF/plugin.definition\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {console/jar:file:/opt/djl/plugins/management-console-0.24.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin console changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {static-file-plugin/jar:file:/opt/djl/plugins/static-file-plugin-0.24.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin static-file-plugin changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {cache-engines/jar:file:/opt/djl/plugins/cache-0.24.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin cache-engines changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m Loading plugin: {kserve/jar:file:/opt/djl/plugins/kserve-0.24.0.jar!/META-INF/plugin.definition}\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin kserve changed state to INITIALIZED\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin console changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin static-file-plugin changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin cache-engines changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPluginMetaData\u001b[m plugin kserve changed state to ACTIVE reason: plugin ready\n",
      "\u001b[32mINFO \u001b[m \u001b[92mFolderScanPluginManager\u001b[m 4 plug-ins found and loaded.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Found model tinyllama=file:/opt/ml/model/tinyllama/\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Initializing model: tinyllama=file:/opt/ml/model/tinyllama/\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Apply per model settings:\n",
      "\tjob_queue_size: 1000\n",
      "\tbatch_size: 1\n",
      "\tmax_batch_delay: 100\n",
      "\tmax_idle_time: 60\n",
      "\tload_on_devices: *\n",
      "\tengine: Python\n",
      "\toption.entryPoint: djl_python.transformers_neuronx\n",
      "\toption.batch_size: 1\n",
      "\toption.dtype: fp16\n",
      "\toption.load_in_8bit: false\n",
      "\toption.n_positions: 512\n",
      "\toption.tensor_parallel_degree: 2\n",
      "\toption.neuron_optimize_level: 1\n",
      "\toption.model_id: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "\toption.rolling_batch: auto\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPlatform\u001b[m Found matching platform from: jar:file:/usr/local/djl-serving-0.24.0/lib/python-0.24.0.jar!/native/lib/python.properties\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python_engine.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/arg_parser.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/deepspeed.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/encode_decode.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/fastertransformer.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/huggingface.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/inputs.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/np_util.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/outputs.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/pair_list.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/lmi_dist_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/neuron_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/scheduler_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/rolling_batch/vllm_rolling_batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/batch.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/lm_block.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/search_config.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/seq_batch_scheduler.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/seq_batcher.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/seq_batcher_impl.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/step_generation.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/seq_scheduler/utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/service_loader.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/stable-diffusion.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/stable_diffusion_inf2.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/streaming_utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/test_model.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/rolling_batch_test_scripts/efficiency_benchmark.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/rolling_batch_test_scripts/rolling_batch_scheduler.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/rolling_batch_test_scripts/run_rolling_batch_alone.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/rolling_batch_test_scripts/test_rolling_batch_scheduler2.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/rolling_batch_test_scripts/test_scheduler_sharded.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/test_input_output.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/test_scheduler.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/tests/test_scheduler_bloom.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/__init__.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/optimum_neuron_scheduler.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/token_selector.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/transformers_neuronx_scheduler/utils.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyEnv\u001b[m Extracting /djl_python/ts_service_loader.py to cache ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelManager\u001b[m Loading model on Python:[nc0, nc2, nc4, nc6, nc8, nc10, nc12, nc14, nc16, nc18, nc20, nc22]\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model tinyllama (PENDING) on nc(0) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 750157 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model tinyllama on nc(0)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19000 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=0-1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: 120 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19000', '--model-dir', '/opt/ml/model/tinyllama', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '0']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Python engine started.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: \n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading config.json: 100%|??????????| 608/608 [00:00<00:00, 208kB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: \n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading tokenizer_config.json: 100%|??????????| 1.29k/1.29k [00:00<00:00, 484kB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: \n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading tokenizer.model: 100%|??????????| 500k/500k [00:00<00:00, 261MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: \n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading tokenizer.json: 100%|??????????| 1.84M/1.84M [00:01<00:00, 1.19MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading tokenizer.json: 100%|??????????| 1.84M/1.84M [00:01<00:00, 1.19MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: \n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading (?)cial_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading (?)cial_tokens_map.json: 100%|??????????| 551/551 [00:00<00:00, 1.46MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: \n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:   2%|?         | 52.4M/2.20G [00:00<00:04, 456MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:   5%|?         | 105M/2.20G [00:00<00:04, 475MB/s] \n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:   7%|?         | 157M/2.20G [00:00<00:04, 490MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  10%|?         | 210M/2.20G [00:00<00:04, 456MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  12%|??        | 262M/2.20G [00:00<00:04, 448MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  14%|??        | 315M/2.20G [00:00<00:04, 459MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  17%|??        | 367M/2.20G [00:00<00:04, 452MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  19%|??        | 419M/2.20G [00:00<00:03, 459MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  21%|???       | 472M/2.20G [00:01<00:04, 430MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  24%|???       | 524M/2.20G [00:01<00:03, 442MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  26%|???       | 577M/2.20G [00:01<00:03, 441MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  29%|???       | 629M/2.20G [00:01<00:03, 447MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  31%|???       | 682M/2.20G [00:01<00:03, 462MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  33%|????      | 734M/2.20G [00:01<00:03, 450MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  36%|????      | 786M/2.20G [00:01<00:03, 443MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  38%|????      | 839M/2.20G [00:01<00:03, 439MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  41%|????      | 891M/2.20G [00:02<00:03, 418MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  43%|?????     | 944M/2.20G [00:02<00:03, 328MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  45%|?????     | 996M/2.20G [00:02<00:03, 359MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  48%|?????     | 1.05G/2.20G [00:02<00:02, 396MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  50%|?????     | 1.10G/2.20G [00:02<00:02, 426MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  52%|??????    | 1.15G/2.20G [00:02<00:02, 430MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  55%|??????    | 1.21G/2.20G [00:02<00:02, 440MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  57%|??????    | 1.26G/2.20G [00:02<00:02, 459MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  60%|??????    | 1.31G/2.20G [00:03<00:01, 450MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  62%|???????   | 1.36G/2.20G [00:03<00:01, 454MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  64%|???????   | 1.42G/2.20G [00:03<00:01, 434MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  67%|???????   | 1.47G/2.20G [00:03<00:01, 428MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  69%|???????   | 1.52G/2.20G [00:03<00:01, 445MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  71%|????????  | 1.57G/2.20G [00:03<00:01, 453MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  74%|????????  | 1.63G/2.20G [00:03<00:01, 470MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  77%|????????  | 1.69G/2.20G [00:03<00:01, 493MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  80%|????????  | 1.75G/2.20G [00:03<00:00, 503MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  82%|????????? | 1.81G/2.20G [00:04<00:00, 517MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  85%|????????? | 1.88G/2.20G [00:04<00:00, 514MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  88%|????????? | 1.93G/2.20G [00:04<00:00, 500MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  90%|????????? | 1.98G/2.20G [00:04<00:00, 500MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  92%|??????????| 2.03G/2.20G [00:04<00:00, 481MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  95%|??????????| 2.09G/2.20G [00:04<00:00, 455MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors:  97%|??????????| 2.14G/2.20G [00:04<00:00, 421MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors: 100%|??????????| 2.19G/2.20G [00:04<00:00, 446MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading model.safetensors: 100%|??????????| 2.20G/2.20G [00:04<00:00, 447MB/s]\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: \n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stderr: Downloading generation_config.json: 100%|??????????| 124/124 [00:00<00:00, 48.1kB/s]\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: 2024-06-19 13:23:52.000431:  134  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: 2024-06-19 13:23:52.000433:  134  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/no-user/neuroncc_compile_workdir/2d3941f6-98d1-4048-86e6-7ea6b6457bd0/model.MODULE_a5e62e3c494cc4d4bc8e+016d3e7a.hlo.pb', '--output', '/tmp/no-user/neuroncc_compile_workdir/2d3941f6-98d1-4048-86e6-7ea6b6457bd0/model.MODULE_a5e62e3c494cc4d4bc8e+016d3e7a.neff', '--logfile', '/tmp/compile.log', '--temp-dir=/tmp', '--model-type=transformer', '-O1', '--model-type=transformer', '--verbose=35']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: 2024-06-19 13:23:52.000458:  135  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: 2024-06-19 13:23:52.000460:  135  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/no-user/neuroncc_compile_workdir/70300fa7-6c97-4ee0-a3dc-18b0c2f6ff30/model.MODULE_e7507bf770860b681403+016d3e7a.hlo.pb', '--output', '/tmp/no-user/neuroncc_compile_workdir/70300fa7-6c97-4ee0-a3dc-18b0c2f6ff30/model.MODULE_e7507bf770860b681403+016d3e7a.neff', '--logfile', '/tmp/compile.log', '--temp-dir=/tmp', '--model-type=transformer', '-O1', '--model-type=transformer', '--verbose=35']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: 2024-06-19 13:23:52.000485:  136  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: 2024-06-19 13:23:52.000487:  136  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/no-user/neuroncc_compile_workdir/c6c2c468-0096-40ab-bd37-5943a5f745dc/model.MODULE_8972fee769e3352ff5a3+016d3e7a.hlo.pb', '--output', '/tmp/no-user/neuroncc_compile_workdir/c6c2c468-0096-40ab-bd37-5943a5f745dc/model.MODULE_8972fee769e3352ff5a3+016d3e7a.neff', '--logfile', '/tmp/compile.log', '--temp-dir=/tmp', '--model-type=transformer', '-O1', '--model-type=transformer', '--verbose=35']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: ......\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Compiler status PASS\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: \n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Compiler status PASS\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: \n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Compiler status PASS\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: 2024-06-19 13:24:17.000303:  790  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: 2024-06-19 13:24:17.000305:  790  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/no-user/neuroncc_compile_workdir/b1acdb04-7478-4467-80a9-d702aa72c8b9/model.MODULE_b91b69273f8cd1de8fcf+016d3e7a.hlo.pb', '--output', '/tmp/no-user/neuroncc_compile_workdir/b1acdb04-7478-4467-80a9-d702aa72c8b9/model.MODULE_b91b69273f8cd1de8fcf+016d3e7a.neff', '--logfile', '/tmp/compile.log', '--temp-dir=/tmp', '--model-type=transformer', '-O1', '--model-type=transformer', '--verbose=35']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: .\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Compiler status PASS\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: 2024-06-19 13:24:25.000879:  1009  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: 2024-06-19 13:24:25.000882:  1009  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/no-user/neuroncc_compile_workdir/376d9c71-7b96-4599-acd3-2ca36bba8892/model.MODULE_869cd543241d0405b9eb+016d3e7a.hlo.pb', '--output', '/tmp/no-user/neuroncc_compile_workdir/376d9c71-7b96-4599-acd3-2ca36bba8892/model.MODULE_869cd543241d0405b9eb+016d3e7a.neff', '--logfile', '/tmp/compile.log', '--temp-dir=/tmp', '--model-type=transformer', '-O1', '--model-type=transformer', '--verbose=35']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: .\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Compiler status PASS\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: 2024-06-19 13:24:35.000067:  1228  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: 2024-06-19 13:24:35.000069:  1228  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: ['neuronx-cc', '--target=trn1', 'compile', '--framework', 'XLA', '/tmp/no-user/neuroncc_compile_workdir/61039fab-31be-4fac-ad45-12e97bfb1a7e/model.MODULE_b3c4cedbdc8ed79e1006+016d3e7a.hlo.pb', '--output', '/tmp/no-user/neuroncc_compile_workdir/61039fab-31be-4fac-ad45-12e97bfb1a7e/model.MODULE_b3c4cedbdc8ed79e1006+016d3e7a.neff', '--logfile', '/tmp/compile.log', '--temp-dir=/tmp', '--model-type=transformer', '-O1', '--model-type=transformer', '--verbose=35']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: .\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Compiler status PASS\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: SysHealth: LLM sharding and compilation latency: 62.99725866317749 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [tinyllama] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model tinyllama (READY) on nc(2) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 744602 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model tinyllama on nc(2)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19001 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=2-3\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: 1450 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19001', '--model-dir', '/opt/ml/model/tinyllama', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '2']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: 2024-06-19 13:25:00.000709:  1464  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: 2024-06-19 13:25:00.000716:  1464  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: 2024-06-19 13:25:00.000736:  1465  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: 2024-06-19 13:25:00.000743:  1465  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: 2024-06-19 13:25:00.000762:  1466  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: 2024-06-19 13:25:00.000769:  1466  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: 2024-06-19 13:25:01.000853:  1475  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: 2024-06-19 13:25:01.000862:  1475  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: 2024-06-19 13:25:02.000865:  1480  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: 2024-06-19 13:25:02.000875:  1480  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: 2024-06-19 13:25:03.000890:  1485  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: 2024-06-19 13:25:03.000903:  1485  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: SysHealth: LLM sharding and compilation latency: 14.292545080184937 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [tinyllama] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model tinyllama (READY) on nc(4) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 739318 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model tinyllama on nc(4)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19002 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=4-5\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: 1492 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19002', '--model-dir', '/opt/ml/model/tinyllama', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '4']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: 2024-06-19 13:25:20.000171:  1505  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: 2024-06-19 13:25:20.000178:  1505  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: 2024-06-19 13:25:20.000196:  1506  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: 2024-06-19 13:25:20.000203:  1506  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: 2024-06-19 13:25:20.000228:  1507  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: 2024-06-19 13:25:20.000238:  1507  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: 2024-06-19 13:25:21.000300:  1516  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: 2024-06-19 13:25:21.000309:  1516  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: 2024-06-19 13:25:22.000286:  1521  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: 2024-06-19 13:25:22.000297:  1521  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: 2024-06-19 13:25:23.000311:  1526  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: 2024-06-19 13:25:23.000324:  1526  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: SysHealth: LLM sharding and compilation latency: 14.250253200531006 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [tinyllama] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model tinyllama (READY) on nc(6) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 733846 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model tinyllama on nc(6)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19003 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=6-7\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: 1533 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19003', '--model-dir', '/opt/ml/model/tinyllama', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '6']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: 2024-06-19 13:25:39.000366:  1546  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: 2024-06-19 13:25:39.000373:  1546  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: 2024-06-19 13:25:39.000392:  1547  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: 2024-06-19 13:25:39.000399:  1547  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: 2024-06-19 13:25:39.000417:  1548  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: 2024-06-19 13:25:39.000424:  1548  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: 2024-06-19 13:25:40.000534:  1558  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: 2024-06-19 13:25:40.000543:  1558  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: 2024-06-19 13:25:41.000586:  1563  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: 2024-06-19 13:25:41.000596:  1563  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: 2024-06-19 13:25:42.000695:  1568  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: 2024-06-19 13:25:42.000708:  1568  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: SysHealth: LLM sharding and compilation latency: 14.429391860961914 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [tinyllama] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model tinyllama (READY) on nc(8) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 728412 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model tinyllama on nc(8)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19004 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=8-9\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: 1575 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19004', '--model-dir', '/opt/ml/model/tinyllama', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '8']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: 2024-06-19 13:25:59.000359:  1587  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: 2024-06-19 13:25:59.000365:  1587  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: 2024-06-19 13:25:59.000384:  1588  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: 2024-06-19 13:25:59.000391:  1588  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: 2024-06-19 13:25:59.000410:  1589  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: 2024-06-19 13:25:59.000417:  1589  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: 2024-06-19 13:26:00.000500:  1598  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: 2024-06-19 13:26:00.000509:  1598  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: 2024-06-19 13:26:01.000550:  1603  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: 2024-06-19 13:26:01.000560:  1603  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: 2024-06-19 13:26:02.000645:  1608  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: 2024-06-19 13:26:02.000658:  1608  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: SysHealth: LLM sharding and compilation latency: 14.52103567123413 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [tinyllama] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model tinyllama (READY) on nc(10) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 722959 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model tinyllama on nc(10)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19005 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=10-11\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: 1615 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19005', '--model-dir', '/opt/ml/model/tinyllama', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '10']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: 2024-06-19 13:26:18.000799:  1627  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: 2024-06-19 13:26:18.000806:  1627  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: 2024-06-19 13:26:18.000826:  1628  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: 2024-06-19 13:26:18.000833:  1628  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: 2024-06-19 13:26:18.000851:  1629  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: 2024-06-19 13:26:18.000858:  1629  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: 2024-06-19 13:26:19.000948:  1638  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: 2024-06-19 13:26:19.000958:  1638  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: 2024-06-19 13:26:20.000939:  1643  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: 2024-06-19 13:26:20.000950:  1643  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: 2024-06-19 13:26:22.000015:  1648  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: 2024-06-19 13:26:22.000027:  1648  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: SysHealth: LLM sharding and compilation latency: 14.319673538208008 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [tinyllama] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model tinyllama (READY) on nc(12) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 717511 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model tinyllama on nc(12)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19006 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=12-13\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: 1655 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19006', '--model-dir', '/opt/ml/model/tinyllama', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '12']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: 2024-06-19 13:26:38.000293:  1668  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: 2024-06-19 13:26:38.000300:  1668  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: 2024-06-19 13:26:38.000319:  1669  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: 2024-06-19 13:26:38.000326:  1669  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: 2024-06-19 13:26:38.000345:  1670  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: 2024-06-19 13:26:38.000352:  1670  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: 2024-06-19 13:26:39.000431:  1679  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: 2024-06-19 13:26:39.000441:  1679  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: 2024-06-19 13:26:40.000473:  1684  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: 2024-06-19 13:26:40.000483:  1684  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: 2024-06-19 13:26:41.000566:  1689  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: 2024-06-19 13:26:41.000578:  1689  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: SysHealth: LLM sharding and compilation latency: 14.420970678329468 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [tinyllama] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model tinyllama (READY) on nc(14) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 712441 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model tinyllama on nc(14)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19007 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=14-15\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: 1696 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19007', '--model-dir', '/opt/ml/model/tinyllama', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '14']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: 2024-06-19 13:26:57.000719:  1708  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: 2024-06-19 13:26:57.000726:  1708  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: 2024-06-19 13:26:57.000746:  1709  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: 2024-06-19 13:26:57.000752:  1709  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: 2024-06-19 13:26:57.000772:  1710  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: 2024-06-19 13:26:57.000779:  1710  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: 2024-06-19 13:26:58.000861:  1719  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: 2024-06-19 13:26:58.000870:  1719  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: 2024-06-19 13:26:59.000954:  1724  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: 2024-06-19 13:26:59.000964:  1724  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: 2024-06-19 13:27:00.000972:  1729  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: 2024-06-19 13:27:00.000985:  1729  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: SysHealth: LLM sharding and compilation latency: 14.442343711853027 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [tinyllama] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model tinyllama (READY) on nc(16) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 706980 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model tinyllama on nc(16)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19008 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=16-17\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: 1738 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19008', '--model-dir', '/opt/ml/model/tinyllama', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '16']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: 2024-06-19 13:27:17.000197:  1750  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: 2024-06-19 13:27:17.000204:  1750  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: 2024-06-19 13:27:17.000224:  1751  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: 2024-06-19 13:27:17.000231:  1751  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: 2024-06-19 13:27:17.000250:  1752  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: 2024-06-19 13:27:17.000257:  1752  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: 2024-06-19 13:27:18.000347:  1761  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: 2024-06-19 13:27:18.000357:  1761  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: 2024-06-19 13:27:19.000405:  1766  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: 2024-06-19 13:27:19.000416:  1766  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: 2024-06-19 13:27:20.000469:  1771  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: 2024-06-19 13:27:20.000482:  1771  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: SysHealth: LLM sharding and compilation latency: 14.482372522354126 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [tinyllama] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model tinyllama (READY) on nc(18) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 701510 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model tinyllama on nc(18)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19009 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=18-19\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: 1778 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19009', '--model-dir', '/opt/ml/model/tinyllama', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '18']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: 2024-06-19 13:27:36.000644:  1805  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: 2024-06-19 13:27:36.000650:  1805  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: 2024-06-19 13:27:36.000670:  1806  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: 2024-06-19 13:27:36.000677:  1806  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: 2024-06-19 13:27:36.000697:  1807  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: 2024-06-19 13:27:36.000703:  1807  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: 2024-06-19 13:27:37.000797:  1816  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: 2024-06-19 13:27:37.000807:  1816  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: 2024-06-19 13:27:38.000783:  1821  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: 2024-06-19 13:27:38.000793:  1821  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: 2024-06-19 13:27:39.000860:  1826  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: 2024-06-19 13:27:39.000873:  1826  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: SysHealth: LLM sharding and compilation latency: 14.287862062454224 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [tinyllama] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model tinyllama (READY) on nc(20) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 696037 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model tinyllama on nc(20)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19010 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=20-21\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: 1833 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19010', '--model-dir', '/opt/ml/model/tinyllama', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '20']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: 2024-06-19 13:27:56.000623:  1845  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: 2024-06-19 13:27:56.000629:  1845  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: 2024-06-19 13:27:56.000649:  1846  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: 2024-06-19 13:27:56.000656:  1846  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: 2024-06-19 13:27:56.000676:  1847  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: 2024-06-19 13:27:56.000683:  1847  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: 2024-06-19 13:27:57.000761:  1856  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: 2024-06-19 13:27:57.000771:  1856  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: 2024-06-19 13:27:58.000835:  1861  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: 2024-06-19 13:27:58.000846:  1861  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: 2024-06-19 13:27:59.000926:  1866  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: 2024-06-19 13:27:59.000939:  1866  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: SysHealth: LLM sharding and compilation latency: 14.286891460418701 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [tinyllama] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m loading model tinyllama (READY) on nc(22) ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Available CPU memory: 690583 MB, required: 0 MB, reserved: 500 MB\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelInfo\u001b[m Loading model tinyllama on nc(22)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mWorkerPool\u001b[m scaling up min workers by 1 (from 0 to 1) workers. Total range is min 1 to max 1\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Start process: 19011 - retry: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mConnection\u001b[m Set NEURON_RT_VISIBLE_CORES=22-23\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: 1873 - djl_python_engine started with args: ['--sock-type', 'unix', '--sock-name', '/tmp/djl_sock.19011', '--model-dir', '/opt/ml/model/tinyllama', '--entry-point', 'djl_python.transformers_neuronx', '--device-id', '22']\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Letting libtpu.so load fail during _XLAC import. libtpu.so will be loaded from `libtpu` Python package when the ComputationClient is created.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Python engine started.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Start loading the model TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stderr: /usr/local/lib/python3.8/dist-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stderr:   deprecate(\n",
      "\u001b[33mWARN \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stderr: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Transferring weights from HF to INF2 in-memory\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: LLM sharding and compiling Started ...\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: 2024-06-19 13:28:16.000052:  1885  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: 2024-06-19 13:28:16.000059:  1885  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_a5e62e3c494cc4d4bc8e+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: 2024-06-19 13:28:16.000077:  1886  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: 2024-06-19 13:28:16.000084:  1886  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_e7507bf770860b681403+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: 2024-06-19 13:28:16.000104:  1887  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: 2024-06-19 13:28:16.000111:  1887  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_8972fee769e3352ff5a3+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: 2024-06-19 13:28:17.000193:  1896  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: 2024-06-19 13:28:17.000203:  1896  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b91b69273f8cd1de8fcf+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: 2024-06-19 13:28:18.000168:  1901  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: 2024-06-19 13:28:18.000178:  1901  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_869cd543241d0405b9eb+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: 2024-06-19 13:28:19.000281:  1906  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: 2024-06-19 13:28:19.000293:  1906  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.11.0.34+c5231f848/MODULE_b3c4cedbdc8ed79e1006+016d3e7a/model.neff. Exiting with a successfully compiled graph.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: SysHealth: LLM sharding and compilation latency: 14.44467830657959 secs\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m Model [tinyllama] initialized.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m Initialize BOTH server with: EpollServerSocketChannel.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mModelServer\u001b[m BOTH API bind to: http://0.0.0.0:8080\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Parse input failed: 0\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Traceback (most recent call last):\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:   File \"/tmp/.djl.ai/python/0.24.0/djl_python/transformers_neuronx.py\", line 268, in parse_input\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:     input_map = decode(item, content_type)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:   File \"/tmp/.djl.ai/python/0.24.0/djl_python/encode_decode.py\", line 55, in decode\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:     return inputs.get_as_json(key=key)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:   File \"/tmp/.djl.ai/python/0.24.0/djl_python/inputs.py\", line 192, in get_as_json\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:     return json.loads((self.get_as_bytes(key=key).decode(\"utf-8\")))\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:   File \"/usr/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:     return _default_decoder.decode(s)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:   File \"/usr/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:     obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:   File \"/usr/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:     obj, end = self.scan_once(s, idx)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 2 column 11 (char 29)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Rolling batch inference error\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Traceback (most recent call last):\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:   File \"/tmp/.djl.ai/python/0.24.0/djl_python/rolling_batch/rolling_batch.py\", line 114, in try_catch_handling\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:     return func(self, input_data, parameters)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:   File \"/tmp/.djl.ai/python/0.24.0/djl_python/rolling_batch/neuron_rolling_batch.py\", line 42, in inference\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:     generations = self.scheduler.decode()\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:   File \"/tmp/.djl.ai/python/0.24.0/djl_python/transformers_neuronx_scheduler/optimum_neuron_scheduler.py\", line 275, in decode\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout:     raise ValueError(\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: ValueError: Unable to decode tokens for non-prefilled batches (probably due to a previous failure)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=32) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=32) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=32) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=32) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=32) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1778-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1833-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1873-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-120-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1450-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1492-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1533-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1575-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1615-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1655-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1696-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Both `max_new_tokens` (=30) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "\u001b[32mINFO \u001b[m \u001b[92mPyProcess\u001b[m W-1738-tinyllama-stdout: Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "!docker run -it --rm  --rm --network=host \\\n",
    "  -v `pwd`/models:/opt/ml/model/ \\\n",
    "  -v `pwd`/logs:/opt/djl/logs \\\n",
    "  -v `pwd`/TinyLlama-neuron:/models/TinyLlama-neuron \\\n",
    "  -u djl \\\n",
    "  --device /dev/neuron0 \\\n",
    "  --device /dev/neuron1 \\\n",
    "  --device /dev/neuron2 \\\n",
    "  --device /dev/neuron3 \\\n",
    "  --device /dev/neuron4 \\\n",
    "  --device /dev/neuron5 \\\n",
    "  --device /dev/neuron6 \\\n",
    "  --device /dev/neuron7 \\\n",
    "  --device /dev/neuron8 \\\n",
    "  --device /dev/neuron9 \\\n",
    "  --device /dev/neuron10 \\\n",
    "  --device /dev/neuron11 \\\n",
    "  -e MODEL_LOADING_TIMEOUT=7200 \\\n",
    "  -e PREDICT_TIMEOUT=360 \\\n",
    "  {image_uri} serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a61a226-3178-45a6-a2fb-9f7bec510370",
   "metadata": {},
   "source": [
    "## 추론 테스트\n",
    "Docker 엔드포인트가 생성된 후, Predictor 객체를 사용하여 Docker 엔드포인트에 대해 실시간 예측을 수행할 수 있습니다.\n",
    "- 추론 요청을 제출하고 응답을 받기 위해 `curl` 명령어를 사용합니다.\n",
    "- 요청과 응답은 JSON 형식으로 이루어집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cac86-9b46-4d78-a914-2d0c82f58ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "curl -X POST \"http://127.0.0.1:8080/predictions/tinyllama\" \\\n",
    "     -H 'Content-Type: application/json' \\\n",
    "     -d '{\n",
    "         \"seq_length\": 512,\n",
    "         \"inputs\": \"Welcome to Amazon Elastic Compute Cloud\",\n",
    "         \"parameters\": {\n",
    "             \"max_new_tokens\": 32,\n",
    "             \"do_sample\": \"true\"\n",
    "         }\n",
    "     }'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4977b17d-dabc-45ab-89e9-43621f9ac7fe",
   "metadata": {},
   "source": [
    "터미널을 열고 모델 서버에 추론 요청을 제출하고 추론 결과를 받아봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda43d8-287f-44d6-93ad-e8b88d3b95b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
